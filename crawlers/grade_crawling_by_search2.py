# -*- coding: utf-8 -*-
# """
# NICE ì‹ ìš©í‰ê°€ - ê²€ìƒ‰ ê²°ê³¼ê°€ 'ëª©ë¡(í‘œ)'ë¡œ ëœ¨ëŠ” ê²½ìš°ê¹Œì§€ ì²˜ë¦¬
# - ê²€ìƒ‰ í›„ ë°”ë¡œ ìƒì„¸(companyGradeInfo)ë¡œ ì´ë™í•˜ë©´ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
# - ê²€ìƒ‰ ê²°ê³¼ê°€ /search/search.do (ê¸°ì—… ëª©ë¡ í‘œ)ë¡œ ëœ¨ë©´,
#   í‘œì˜ 'ì±„ê¶Œ' ì¹¼ëŸ¼ì— ê°’ì´ ìˆëŠ” ê¸°ì—…ë“¤ë§Œ ìˆœì„œëŒ€ë¡œ cmpCdë¥¼ ìˆ˜ì§‘í•œ ë’¤ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë“±ê¸‰ íŒŒì‹±
# - íšŒì‚¬ëª…ì€ ìƒì„¸ í˜ì´ì§€ì—ì„œ í™•ì • ì¶”ì¶œ(div.tbl_type99 > table) â†’ ì •í™•ë„ ë³´ì¥
# """

import os
import re
import time
import unicodedata
from datetime import datetime
from difflib import SequenceMatcher
from urllib.parse import urlparse, parse_qs

import requests
import pandas as pd
from bs4 import BeautifulSoup

# Selenium
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager


# -------------------- ê¸°ë³¸ ì„¤ì • --------------------
BASE = "https://www.nicerating.com"
HOME = f"{BASE}/"
SEARCH_TIMEOUT = 20
REQUEST_TIMEOUT = 15
HEADLESS = True  # ë””ë²„ê¹… ì‹œ False

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/118.0.0.0 Safari/537.36"
    )
}

# -------------------- ì´ë¦„/ë“±ê¸‰ íŒŒì„œ (ìƒì„¸ í˜ì´ì§€) --------------------
def _extract_name_from_tbl_type99(soup: BeautifulSoup) -> str | None:
    tbl = soup.select_one("div.tbl_type99 table")
    if not tbl:
        return None
    tbody = tbl.find("tbody")
    first_td = (tbody.find("td") if tbody else tbl.find("td"))
    return first_td.get_text(" ", strip=True) if first_td else None

def _extract_grade_primary_tbl1(soup: BeautifulSoup) -> str | None:
    table = soup.find("table", {"id": "tbl1"})
    if not table:
        return None
    tds = table.find_all("td", class_="cell_txt01")
    if not tds:
        return None
    return tds[0].get_text(strip=True) or None

LONGTERM_GRADE_RE = re.compile(
    r"^(AAA|AA\+|AA|AA\-|A\+|A|A\-|BBB\+|BBB|BBB\-|BB\+|BB|BB\-|B\+|B|B\-|CCC|CC|C|D)$"
)
OUTLOOK_RE = re.compile(r"(ì•ˆì •ì |ê¸ì •ì |ë¶€ì •ì |ìœ ë™ì |Stable|Positive|Negative|Developing)", re.I)

def _extract_grade_from_major_table(soup: BeautifulSoup) -> str | None:
    """ìƒì„¸ í˜ì´ì§€ 'ì£¼ìš” ë“±ê¸‰ë‚´ì—­'ì˜ 'íšŒì‚¬ì±„' í–‰ì—ì„œ í˜„ì¬ ë“±ê¸‰/ì „ë§ ë³´ê°• ì¶”ì¶œ."""
    for tr in soup.find_all("tr"):
        text = tr.get_text(" ", strip=True)
        if "íšŒì‚¬ì±„" in text:
            toks = text.replace("/", " ").split()
            grades = [x for x in toks if LONGTERM_GRADE_RE.match(x)]
            outs   = [x for x in toks if OUTLOOK_RE.search(x)]
            if grades:
                g = grades[-1]
                return f"{g} {outs[-1]}" if outs else g
    return None

def fetch_company_and_grade_by_cmpcd(cmpCd: str, session: requests.Session) -> tuple[str, str]:
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ íšŒì‚¬ëª…/ì±„ê¶Œ ë“±ê¸‰ íŒŒì‹±(ì›ë˜ í•˜ë˜ ë°©ì‹)."""
    url = f"{BASE}/disclosure/companyGradeInfo.do?cmpCd={cmpCd}&deviceType=N&isPaidMember=false"
    resp = session.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    # íšŒì‚¬ëª…
    name = _extract_name_from_tbl_type99(soup)
    if not name:
        for sel in ["div.cont_title h3", "h3.tit", ".company_name", ".cmp_name", "div.title_area h3"]:
            el = soup.select_one(sel)
            if el:
                name = el.get_text(strip=True)
                break
    if not name:
        title_text = soup.title.get_text(strip=True) if soup.title else ""
        name = title_text.split("|")[0].split("-")[0].strip() or cmpCd

    # ë“±ê¸‰
    grade = _extract_grade_primary_tbl1(soup) or _extract_grade_from_major_table(soup) or ""
    return name, grade


# -------------------- ë¬¸ìì—´ ë³´ì •/ìœ ì‚¬ë„ --------------------
ALIASES = [
    (re.compile(r"\bì—ìŠ¤ì¼€ì´", re.I), "SK"),
    (re.compile(r"\bì—˜ì§€", re.I), "LG"),
    (re.compile(r"\bì”¨ì œì´", re.I), "CJ"),
    (re.compile(r"\bì¼€ì´í‹°\b", re.I), "KT"),
    (re.compile(r"\bì¼€ì´ë¹„\b", re.I), "KB"),
    (re.compile(r"\bì—”ì—ì´ì¹˜\b", re.I), "NH"),
    (re.compile(r"\bì—ì´ì¹˜ë””", re.I), "HD"),
    (re.compile(r"\bì—ì´ì¼€ì´\b", re.I), "AK"),
    (re.compile(r"\bí¬ìŠ¤ì½”", re.I), "POSCO"),
    (re.compile(r"\bì”¨ì œì´", re.I), "CJ"),
    (re.compile(r"\bì¼€ì´ë¹„", re.I), "KB"),
    (re.compile(r"\bì§€ì—ìŠ¤", re.I), "GS"),
    (re.compile(r"\bì•„ì´ë¹„", re.I), "IB"),
    (re.compile(r"\bë¹„ì—‘ìŠ¤", re.I), "BX"),
    (re.compile(r"\bì—ì´ì•„ì´", re.I), "AI"),

]
def aliasize(s: str) -> str:
    for pat, rep in ALIASES:
        s = pat.sub(rep, s)
    return re.sub(r"\s*\(ì£¼\)|ãˆœ", "", s).strip()

def normalize_text(s: str) -> str:
    if not s: return ""
    s = unicodedata.normalize("NFKC", s).strip().lower()
    s = re.sub(r"(ì£¼ì‹íšŒì‚¬|ãˆœ|\(ì£¼\)|ìœ í•œíšŒì‚¬|í™€ë”©ìŠ¤)", " ", s)
    s = re.sub(r"[\(\)\[\]\{\}]", " ", s)
    return re.sub(r"\s+", " ", s)

def name_similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, normalize_text(a), normalize_text(b)).ratio()


# -------------------- ê²€ìƒ‰/í˜ì´ì§€ ì „í™˜ ìœ í‹¸ --------------------
def _stealth_options() -> Options:
    opts = Options()
    if HEADLESS: opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--lang=ko-KR")
    opts.add_argument("--window-size=1280,2000")
    opts.add_argument(f"user-agent={HEADERS['User-Agent']}")
    # ë´‡ íŠ¹ì„± ì œê±°
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    return opts

def _post_launch_stealth(driver):
    try:
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {"source": 'Object.defineProperty(navigator, "webdriver", {get: () => undefined});'}
        )
    except Exception:
        pass

def _find_search_box(driver: webdriver.Chrome):
    sels = [
        "input#searchKeyword", "input#keyword",
        "input[name='searchKeyword']", "input[name='keyword']",
        "header input[type='text']", "form input[type='text']",
        "input[type='search']",
    ]
    for sel in sels:
        try:
            el = WebDriverWait(driver, 6).until(EC.presence_of_element_located((By.CSS_SELECTOR, sel)))
            if el.is_displayed():
                return el
        except Exception:
            pass
    raise RuntimeError("ê²€ìƒ‰ ì…ë ¥ì°½ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

def _close_overlays(driver):
    for sel in ["button.close", ".btn_close", "a.close", "button[aria-label='ë‹«ê¸°']"]:
        try:
            for el in driver.find_elements(By.CSS_SELECTOR, sel):
                if el.is_displayed():
                    el.click()
        except Exception:
            pass

def _parse_cmpcd_from_url(url: str) -> str | None:
    try:
        q = parse_qs(urlparse(url).query)
        if "cmpCd" in q and q["cmpCd"]:
            return q["cmpCd"][0]
    except Exception:
        pass
    m = re.search(r"cmpCd=(\d+)", url)
    return m.group(1) if m else None


# -------------------- ê²€ìƒ‰ ê²°ê³¼ 'ëª©ë¡(í‘œ)' íŒŒì„œ --------------------
# cmpCd íŒ¨í„´(ë§í¬/onclick/ì†ì„± ëª¨ë‘)
CMP_RE = re.compile(r"cmpCd=(\d+)")
JS_CMP_RE = re.compile(r"fn_cmpGradeInfo\('(\d+)'\)")
ANY_CODE_RE = re.compile(r"(\d{7,8})")  # ì•ˆì „ ì¥ì¹˜: 7~8ìë¦¬ ìˆ«ì

# ì±„ê¶Œ ì¹¼ëŸ¼ í…ìŠ¤íŠ¸ ì¸ì§€(ì¥ê¸°ë“±ê¸‰/ì „ë§; A1ê°™ì€ CPëŠ” ë°°ì œ)
GRADE_CELL_RE = re.compile(
    r"(AAA|AA\+|AA|AA-|A\+|A|A-|BBB\+|BBB|BBB-|BB\+|BB|BB-|B\+|B|B-|CCC|CC|C|D)"
    r"(?:\s*/\s*(Stable|Positive|Negative|Developing|ì•ˆì •ì |ê¸ì •ì |ë¶€ì •ì |ìœ ë™ì ))?",
    re.I
)

def _extract_candidates_from_search_table(html: str) -> list[dict]:
    """
    /search/search.do ê²°ê³¼ í…Œì´ë¸”ì—ì„œ
    - cmpCd
    - name_hint(í–‰ ë‚´ í…ìŠ¤íŠ¸)
    - list_grade(ì±„ê¶Œ ì¹¼ëŸ¼ ì¶”ì • ê°’)
    ì„ ë½‘ëŠ”ë‹¤. (í‘œì˜ ìˆœì„œ ë³´ì¡´)
    """
    soup = BeautifulSoup(html, "html.parser")

    # í‘œ(í´ë˜ìŠ¤ê°€ ë°”ë€Œì–´ë„ ì²« ë²ˆì§¸ í…Œì´ë¸”ì„ ê¸°ë³¸ìœ¼ë¡œ ì‹œë„)
    table = soup.select_one("div.tbl_type01 table") or soup.find("table")
    if not table:
        return []

    out: list[dict] = []
    tbody = table.find("tbody") or table

    for tr in tbody.find_all("tr", recursive=False):
        tds = tr.find_all("td")
        if not tds:
            continue

        # 1) cmpCd ì¶”ì¶œ (í–‰ ë‚´ë¶€ì˜ href/onclick/ì†ì„± ì „ì²´ ìŠ¤ìº”)
        cmpcd = None
        # a[href]
        for a in tr.select('a[href]'):
            href = a.get("href") or ""
            m = CMP_RE.search(href) or JS_CMP_RE.search(href)
            if m:
                cmpcd = m.group(1); break
        if not cmpcd:
            # onclickë“¤
            for el in tr.find_all(onclick=True):
                oc = el.get("onclick") or ""
                m = JS_CMP_RE.search(oc) or CMP_RE.search(oc) or ANY_CODE_RE.search(oc)
                if m:
                    code = m.group(1)
                    if len(code) >= 7:
                        cmpcd = code; break
        if not cmpcd:
            # ëª¨ë“  ì†ì„±ì—ì„œ ê²€ìƒ‰ (í¬ê·€)
            for el in tr.find_all(True):
                for k, v in el.attrs.items():
                    if isinstance(v, str):
                        m = JS_CMP_RE.search(v) or CMP_RE.search(v) or ANY_CODE_RE.search(v)
                        if m:
                            code = m.group(1)
                            if len(code) >= 7:
                                cmpcd = code; break
                if cmpcd:
                    break

        # 2) name_hint (ê¸°ì—…ëª… ì¶”ì • - ìƒì„¸ì—ì„œ í™•ì •í•˜ì§€ë§Œ, ë¡œê¹…ìš©)
        name_hint = ""
        a_name = tr.select_one('a[href*="companyGradeInfo.do"], a[href^="javascript:fn_cmpGradeInfo"]')
        if a_name:
            name_hint = a_name.get_text(" ", strip=True)
        if not name_hint:
            # ì²« ë²ˆì§¸ ë¹„ì–´ìˆì§€ ì•Šì€ td í…ìŠ¤íŠ¸
            for td in tds:
                txt = td.get_text(" ", strip=True)
                if txt:
                    name_hint = txt; break

        # 3) 'ì±„ê¶Œ' ì¹¼ëŸ¼ ê°’ ì¶”ì •
        list_grade = None
        for td in tds:
            text = td.get_text(" ", strip=True)
            # A1 ê°™ì€ CP ë“±ê¸‰ì€ ë°°ì œ(ì¥ê¸°ë“±ê¸‰ íŒ¨í„´ë§Œ í—ˆìš©)
            m = GRADE_CELL_RE.search(text)
            if m:
                g = m.group(1)
                o = m.group(2)
                list_grade = f"{g} {o}" if o else g
                break

        # í•„í„°: 'ì±„ê¶Œ' ê°’ì´ ìˆëŠ” í–‰ë§Œ ì‚¬ìš©
        if cmpcd and list_grade:
            out.append({"cmpCd": cmpcd, "name_hint": name_hint, "list_grade": list_grade})

    return out


# -------------------- ê²€ìƒ‰ ì‹¤í–‰ â†’ ìƒì„¸/ëª©ë¡ ë¶„ê¸° --------------------
def search_and_collect(driver: webdriver.Chrome, query: str, session: requests.Session) -> list[dict]:
    """
    í•˜ë‚˜ì˜ ê²€ìƒ‰ì–´(query)ì— ëŒ€í•´
    - ë°”ë¡œ ìƒì„¸í˜ì´ì§€ë¡œ ì´ë™: cmpCd íŒŒì‹± â†’ ìƒì„¸ íŒŒì‹±
    - ëª©ë¡(í‘œ) í˜ì´ì§€ë¡œ ì´ë™: í‘œì—ì„œ cmpCd+ì±„ê¶Œê°’ ìˆëŠ” í–‰ë§Œ ìˆ˜ì§‘ â†’ ê° ìƒì„¸ íŒŒì‹±
    """
    results: list[dict] = []

    # ì›ë¬¸/ë³„ì¹­ 2íšŒ ì‹œë„
    for attempt, q in enumerate([query, aliasize(query)], 1):
        driver.get(HOME)
        time.sleep(1.0)
        _close_overlays(driver)

        box = _find_search_box(driver)
        box.clear(); box.send_keys(q); box.send_keys(Keys.ENTER)

        # ê²°ê³¼ ëŒ€ê¸°
        try:
            WebDriverWait(driver, SEARCH_TIMEOUT).until(
                EC.any_of(
                    EC.url_contains("companyGradeInfo.do"),
                    EC.url_contains("/search/search.do"),
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href*="companyGradeInfo.do"]')),
                    EC.presence_of_element_located((By.CSS_SELECTOR, '[onclick*="fn_cmpGradeInfo("]'))
                )
            )
        except Exception:
            pass

        curr_url = driver.current_url

        # 1) ìƒì„¸ë¡œ ë°”ë¡œ ì´ë™í•œ ê²½ìš°
        cmpcd = _parse_cmpcd_from_url(curr_url)
        if cmpcd:
            name, grade = fetch_company_and_grade_by_cmpcd(cmpcd, session)
            if grade:
                results.append({
                    "ìš”ì²­ê²€ìƒ‰ì–´": query,
                    "íšŒì‚¬ëª…": name,
                    "cmpCd": cmpcd,
                    "ë“±ê¸‰": grade
                })
            return results  # ë°”ë¡œ ìƒì„¸ë¡œ ë“¤ì–´ê°”ìœ¼ë©´ ì¢…ë£Œ

        # 2) ëª©ë¡(í‘œ) í˜ì´ì§€ì¸ ê²½ìš°: í‘œ íŒŒì‹±
        if "/search/search.do" in curr_url:
            html = driver.page_source
            candidates = _extract_candidates_from_search_table(html)
            if not candidates:
                continue  # ë³„ì¹­ ì‹œë„
            # í‘œì˜ ìˆœì„œëŒ€ë¡œ ìƒì„¸ íŒŒì‹±
            for cand in candidates:
                code = cand["cmpCd"]
                name, grade = fetch_company_and_grade_by_cmpcd(code, session)
                if not grade:
                    continue
                results.append({
                    "ìš”ì²­ê²€ìƒ‰ì–´": query,
                    "íšŒì‚¬ëª…": name,
                    "cmpCd": code,
                    "ë“±ê¸‰": grade
                })
            return results

        # 3) ê·¸ ë°–ì˜ í™”ë©´: DOMì—ì„œ ì§ì ‘ í›„ë³´ë¥¼ ì°¾ì•„ ìƒì„¸ë¡œ(ì˜ˆì „ ë¡œì§ í´ë°±)
        html = driver.page_source
        # onclick/hrefì—ì„œ cmpCd ìˆ˜ìƒ‰
        soup = BeautifulSoup(html, "html.parser")
        code = None
        for a in soup.select('a[href*="companyGradeInfo.do"]'):
            m = re.search(r"cmpCd=(\d+)", a.get("href", ""))
            if m: code = m.group(1); break
        if not code:
            for el in soup.find_all(onclick=True):
                m = JS_CMP_RE.search(el.get("onclick",""))
                if m: code = m.group(1); break

        if code:
            name, grade = fetch_company_and_grade_by_cmpcd(code, session)
            if grade:
                results.append({
                    "ìš”ì²­ê²€ìƒ‰ì–´": query,
                    "íšŒì‚¬ëª…": name,
                    "cmpCd": code,
                    "ë“±ê¸‰": grade
                })
            return results

        # alias ì‹œë„ë¡œ ì¬ë„ì „
    return results


# -------------------- I/O & ë©”ì¸ íŒŒì´í”„ë¼ì¸ --------------------
def load_companies_from_txt() -> list[str]:
    base_dir = os.path.dirname(__file__)
    path = os.path.join(base_dir, "..", "data", "input", "companies.txt")
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8-sig") as f:
        names = [ln.strip() for ln in f if ln.strip()]
    return names

def main():
    companies = load_companies_from_txt()
    print(f"ğŸ“‹ ëŒ€ìƒ ê²€ìƒ‰ì–´ ìˆ˜: {len(companies)}")

    options = _stealth_options()
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    _post_launch_stealth(driver)

    session = requests.Session()
    session.headers.update(HEADERS)

    rows, skipped = [], []
    try:
        for i, q in enumerate(companies, 1):
            try:
                found = search_and_collect(driver, q, session)
            except Exception as e:
                print(f"â— ê²€ìƒ‰ ì‹¤íŒ¨: {q} -> {e}")
                skipped.append((q, "ê²€ìƒ‰ ì‹¤íŒ¨")); continue

            if not found:
                print(f"ğŸ” ê²°ê³¼ ì—†ìŒ: {q}")
                skipped.append((q, "ê²°ê³¼ ì—†ìŒ")); continue

            rows.extend(found)
            if i % 5 == 0:
                print(f"  ì§„í–‰ë¥ : {i}/{len(companies)}")
            time.sleep(0.3)
    finally:
        driver.quit()

    if not rows:
        print("âš  ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = pd.DataFrame(rows).drop_duplicates(subset=["cmpCd"]).set_index("íšŒì‚¬ëª…").sort_index()
    print(df.head(10))

    # ì €ì¥: Kiwoom-Bank/data/output/
    base_dir = os.path.dirname(__file__)
    out_dir = os.path.join(base_dir, "..", "data", "output")
    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, f"ë‚˜ì´ìŠ¤_íšŒì‚¬ì±„ë“±ê¸‰_by_search_{datetime.now():%Y%m%d_%H%M}.csv")
    df.to_csv(out_path, encoding="utf-8-sig")
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {out_path}")

    if skipped:
        print("\nâ€” ê±´ë„ˆë›´ ê²€ìƒ‰ì–´(ì‚¬ìœ ) â€”")
        for name, why in skipped:
            print(f"  â€¢ {name}: {why}")

if __name__ == "__main__":
    main()
