# -*- coding: utf-8 -*-
"""
NICE ì‹ ìš©í‰ê°€ - ë³‘ë ¬ ê²€ìƒ‰ í¬ë¡¤ëŸ¬(ë¦¬íŠ¸ë¼ì´ ê°•í™” ë²„ì „)
- 1ë¼ìš´ë“œ: ë³‘ë ¬(ìŠ¤ë ˆë“œ)ë¡œ ê²€ìƒ‰ â†’ ìƒì„¸/ëª©ë¡ ì²˜ë¦¬
- 2ë¼ìš´ë“œ(ìµœì¢… ë¦¬íŠ¸ë¼ì´): 1ë¼ìš´ë“œì—ì„œ 'ê²€ìƒ‰ ì‹¤íŒ¨/ê²°ê³¼ ì—†ìŒ' ê¸°ì—…ë§Œ
  - ë‹¨ì¼ ë“œë¼ì´ë²„(ì§ë ¬)ë¡œ ì¬ê²€ìƒ‰
  - ëŒ€ê¸°ì‹œê°„ ëŠ˜ë¦¼, ëª©ë¡í‘œì˜ 'ì±„ê¶Œ' ì¹¼ëŸ¼ì´ ë¹„ì–´ë„ cmpCdë§Œ ìˆìœ¼ë©´ ìƒì„¸ ì§„ì…
- ìƒì„¸ í˜ì´ì§€ ìš”ì²­ì€ 500/502/503/504/429 ë“± ì¼ì‹œ ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„
- ì›Œì»¤(WebDriver) ì„¸ì…˜ì´ ê¹¨ì§€ë©´ ìë™ ì¬ìƒì„± í›„ 1íšŒ ì¬ì‹œë„
"""

import os
import re
import time
import math
import unicodedata
from datetime import datetime
from difflib import SequenceMatcher
from urllib.parse import urlparse, parse_qs, quote_plus
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import pandas as pd
from bs4 import BeautifulSoup

# Selenium
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

# -------------------- ê¸°ë³¸ ì„¤ì • --------------------
BASE = "https://www.nicerating.com"
HOME = f"{BASE}/"
SEARCH_TIMEOUT = 20           # 1ë¼ìš´ë“œ ê¸°ë³¸ ëŒ€ê¸°(ì´ˆ)
REQUEST_TIMEOUT = 15
HEADLESS = True               # ë””ë²„ê¹… ì‹œ Falseë¡œ
MAX_WORKERS = 4               # 3~6 ê¶Œì¥
BATCH_SIZE_AUTO = True        # Trueë©´ íšŒì‚¬ ìˆ˜/ì›Œì»¤ ìˆ˜ë¡œ ìë™ ë¶„í• 
FINAL_RETRY = True            # ë§ˆì§€ë§‰ ë¦¬íŠ¸ë¼ì´ ë¼ìš´ë“œ ì‹¤í–‰ ì—¬ë¶€
FINAL_RETRY_TIMEOUT = 35      # 2ë¼ìš´ë“œ(ë¦¬íŠ¸ë¼ì´) ëŒ€ê¸°(ì´ˆ): ë” ê¸¸ê²Œ
FINAL_RETRY_SLEEP = 1.2       # ëª©ë¡í‘œ ì¬íŒŒì‹± ëŒ€ê¸°(ì´ˆ) ì¶”ê°€: ë” ê¸¸ê²Œ

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/118.0.0.0 Safari/537.36"
    ),
}

# ì¼ì‹œì  ì„œë²„ ì˜¤ë¥˜(ì¬ì‹œë„ ëŒ€ìƒ)
TRANSIENT_STATUS = {500, 502, 503, 504, 429}

# -------------------- ìƒì„¸ í˜ì´ì§€ íŒŒì„œ(ê¸°ì¡´ ë¡œì§ ìœ ì§€) --------------------
def _extract_name_from_tbl_type99(soup: BeautifulSoup) -> str | None:
    tbl = soup.select_one("div.tbl_type99 table")
    if not tbl:
        return None
    tbody = tbl.find("tbody")
    first_td = (tbody.find("td") if tbody else tbl.find("td"))
    return first_td.get_text(" ", strip=True) if first_td else None

def _extract_grade_primary_tbl1(soup: BeautifulSoup) -> str | None:
    table = soup.find("table", {"id": "tbl1"})
    if not table:
        return None
    tds = table.find_all("td", class_="cell_txt01")
    if not tds:
        return None
    return tds[0].get_text(strip=True) or None

LONGTERM_GRADE_RE = re.compile(
    r"^(AAA|AA\+|AA|AA\-|A\+|A|A\-|BBB\+|BBB|BBB\-|BB\+|BB|BB\-|B\+|B|B\-|CCC|CC|C|D)$"
)
OUTLOOK_RE = re.compile(r"(ì•ˆì •ì |ê¸ì •ì |ë¶€ì •ì |ìœ ë™ì |Stable|Positive|Negative|Developing)", re.I)

def _extract_grade_from_major_table(soup: BeautifulSoup) -> str | None:
    for tr in soup.find_all("tr"):
        text = tr.get_text(" ", strip=True)
        if "íšŒì‚¬ì±„" in text:
            toks = text.replace("/", " ").split()
            grades = [x for x in toks if LONGTERM_GRADE_RE.match(x)]
            outs   = [x for x in toks if OUTLOOK_RE.search(x)]
            if grades:
                g = grades[-1]
                return f"{g} {outs[-1]}" if outs else g
    return None

# -------------------- Requests ì¬ì‹œë„ ìœ í‹¸ --------------------
def _retry_get(session: requests.Session, url: str, max_retries=2, backoff=2) -> requests.Response:
    """
    500/502/503/504/429 ë“± ì¼ì‹œ ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„.
    """
    last_exc = None
    for attempt in range(1, max_retries + 1):
        try:
            resp = session.get(url, headers={**HEADERS, "Referer": HOME}, timeout=REQUEST_TIMEOUT)
            if resp.status_code in TRANSIENT_STATUS:
                raise requests.HTTPError(f"Transient {resp.status_code}", response=resp)
            resp.raise_for_status()
            return resp
        except requests.RequestException as e:
            last_exc = e
            st = getattr(e, "response", None).status_code if hasattr(e, "response") and e.response else None
            if st in TRANSIENT_STATUS and attempt < max_retries:
                print(f"âš ï¸ HTTP {st} ì¬ì‹œë„ {attempt}/{max_retries} â†’ {url}")
                time.sleep(backoff)
                continue
            break
    raise last_exc or RuntimeError("GET failed")

def fetch_company_and_grade_by_cmpcd(cmpCd: str, session: requests.Session) -> tuple[str, str]:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ íšŒì‚¬ëª…/ì±„ê¶Œ ë“±ê¸‰ íŒŒì‹±(ì›ë˜ í•˜ë˜ ë°©ì‹).
    - ì¼ì‹œ ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„
    """
    url = f"{BASE}/disclosure/companyGradeInfo.do?cmpCd={cmpCd}&deviceType=N&isPaidMember=false"
    resp = _retry_get(session, url, max_retries=2, backoff=2)
    soup = BeautifulSoup(resp.text, "html.parser")

    name = _extract_name_from_tbl_type99(soup)
    if not name:
        for sel in ["div.cont_title h3", "h3.tit", ".company_name", ".cmp_name", "div.title_area h3"]:
            el = soup.select_one(sel)
            if el:
                name = el.get_text(strip=True)
                break
    if not name:
        title_text = soup.title.get_text(strip=True) if soup.title else ""
        name = title_text.split("|")[0].split("-")[0].strip() or cmpCd

    grade = _extract_grade_primary_tbl1(soup) or _extract_grade_from_major_table(soup) or ""
    return name, grade

# -------------------- ë¬¸ìì—´ ë³´ì •/ìœ ì‚¬ë„ --------------------
ALIASES = [
    (re.compile(r"\bì—ìŠ¤ì¼€ì´", re.I), "SK"),
    (re.compile(r"\bì—˜ì§€", re.I), "LG"),
    (re.compile(r"\bì”¨ì œì´", re.I), "CJ"),
    (re.compile(r"\bì¼€ì´í‹°\b", re.I), "KT"),
    (re.compile(r"\bì¼€ì´ë¹„\b", re.I), "KB"),
    (re.compile(r"\bì—”ì—ì´ì¹˜\b", re.I), "NH"),
    (re.compile(r"\bì—ì´ì¹˜ë””", re.I), "HD"),
    (re.compile(r"\bì—ì´ì¼€ì´\b", re.I), "AK"),
]
def aliasize(s: str) -> str:
    for pat, rep in ALIASES:
        s = pat.sub(rep, s)
    return re.sub(r"\s*\(ì£¼\)|ãˆœ", "", s).strip()

def normalize_text(s: str) -> str:
    if not s: return ""
    s = unicodedata.normalize("NFKC", s).strip().lower()
    s = re.sub(r"(ì£¼ì‹íšŒì‚¬|ãˆœ|\(ì£¼\)|ìœ í•œíšŒì‚¬|í™€ë”©ìŠ¤)", " ", s)
    s = re.sub(r"[\(\)\[\]\{\}]", " ", s)
    return re.sub(r"\s+", " ", s)

def name_similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, normalize_text(a), normalize_text(b)).ratio()

# -------------------- Selenium ìœ í‹¸ --------------------
def _stealth_options() -> Options:
    opts = Options()
    if HEADLESS: opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--lang=ko-KR")
    opts.add_argument("--window-size=1280,2000")
    opts.add_argument(f"user-agent={HEADERS['User-Agent']}")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    return opts

def _post_launch_stealth(driver):
    try:
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {"source": 'Object.defineProperty(navigator, "webdriver", {get: () => undefined});'}
        )
    except Exception:
        pass

def _parse_cmpcd_from_url(url: str) -> str | None:
    try:
        q = parse_qs(urlparse(url).query)
        if "cmpCd" in q and q["cmpCd"]:
            return q["cmpCd"][0]
    except Exception:
        pass
    m = re.search(r"cmpCd=(\d+)", url)
    return m.group(1) if m else None

# -------------------- ëª©ë¡(í‘œ) íŒŒì„œ --------------------
CMP_RE = re.compile(r"cmpCd=(\d+)")
JS_CMP_RE = re.compile(r"fn_cmpGradeInfo\('(\d+)'\)")
ANY_CODE_RE = re.compile(r"(\d{7,8})")
GRADE_CELL_RE = re.compile(
    r"(AAA|AA\+|AA|AA-|A\+|A|A-|BBB\+|BBB|BBB-|BB\+|BB|BB-|B\+|B|B-|CCC|CC|C|D)"
    r"(?:\s*/\s*(Stable|Positive|Negative|Developing|ì•ˆì •ì |ê¸ì •ì |ë¶€ì •ì |ìœ ë™ì ))?",
    re.I
)

def _extract_candidates_from_search_table(html: str, require_bond=True) -> list[dict]:
    """
    /search/search.do ê²°ê³¼ í…Œì´ë¸”ì—ì„œ
    - cmpCd
    - name_hint
    - list_grade(ì±„ê¶Œ ì¹¼ëŸ¼ ê°’, ìˆìœ¼ë©´)
    ì¶”ì¶œ. require_bond=Falseë©´ 'ì±„ê¶Œ ì¹¼ëŸ¼'ì´ ì—†ì–´ë„ cmpCdë§Œ ìˆìœ¼ë©´ í›„ë³´ë¡œ í¬í•¨.
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.select_one("div.tbl_type01 table") or soup.find("table")
    if not table:
        return []

    out, tbody = [], (table.find("tbody") or table)
    for tr in tbody.find_all("tr", recursive=False):
        tds = tr.find_all("td")
        if not tds:
            continue

        # cmpCd
        cmpcd = None
        for a in tr.select('a[href]'):
            href = a.get("href") or ""
            m = CMP_RE.search(href) or JS_CMP_RE.search(href)
            if m:
                cmpcd = m.group(1); break
        if not cmpcd:
            for el in tr.find_all(onclick=True):
                oc = el.get("onclick") or ""
                m = JS_CMP_RE.search(oc) or CMP_RE.search(oc) or ANY_CODE_RE.search(oc)
                if m and len(m.group(1)) >= 7:
                    cmpcd = m.group(1); break
        if not cmpcd:
            for el in tr.find_all(True):
                for _, v in el.attrs.items():
                    if isinstance(v, str):
                        m = JS_CMP_RE.search(v) or CMP_RE.search(v) or ANY_CODE_RE.search(v)
                        if m and len(m.group(1)) >= 7:
                            cmpcd = m.group(1); break
                if cmpcd: break
        if not cmpcd:
            continue

        # ì´ë¦„ íŒíŠ¸
        name_hint = ""
        a_name = tr.select_one('a[href*="companyGradeInfo.do"], a[href^="javascript:fn_cmpGradeInfo"]')
        if a_name:
            name_hint = a_name.get_text(" ", strip=True)
        if not name_hint:
            for td in tds:
                txt = td.get_text(" ", strip=True)
                if txt:
                    name_hint = txt; break

        # 'ì±„ê¶Œ' ì¹¼ëŸ¼ ê°’(ìˆìœ¼ë©´)
        list_grade = None
        for td in tds:
            text = td.get_text(" ", strip=True)
            m = GRADE_CELL_RE.search(text)
            if m:
                g, o = m.group(1), m.group(2)
                list_grade = f"{g} {o}" if o else g
                break

        if require_bond and not list_grade:
            continue

        out.append({"cmpCd": cmpcd, "name_hint": name_hint, "list_grade": list_grade})

    return out

# -------------------- ê²€ìƒ‰ ì‹¤í–‰(ê²¬ê³ ) --------------------
def _build_search_url(query: str) -> str:
    return f"{BASE}/search/search.do?mainSType=CMP&mainSText={quote_plus(query)}"

def search_and_collect_resilient(
    driver: webdriver.Chrome,
    query: str,
    session: requests.Session,
    wait_timeout: int = SEARCH_TIMEOUT,
    table_wait_extra: float = 0.8,
    require_bond_in_table: bool = True
) -> list[dict]:
    """
    - ê²€ìƒ‰ URLë¡œ ì§ì ‘ ì§„ì…
    - ìƒì„¸ë¡œ ë°”ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë˜ë©´ cmpCd ì¶”ì¶œ í›„ ìƒì„¸ íŒŒì‹±
    - ëª©ë¡(í‘œ)ë©´ í…Œì´ë¸” íŒŒì‹± â†’ (ì˜µì…˜) 'ì±„ê¶Œ' ì¹¼ëŸ¼ ê°’ ìˆëŠ” í–‰ë§Œ ìƒì„¸ ì¬íŒŒì‹±
    - DOM ëŠ¦ê²Œ ëœ° ìˆ˜ ìˆì–´ ì¬íŒŒì‹± 1íšŒ ë” ì‹œë„
    """
    results: list[dict] = []
    candidates_query = [
        query,
        aliasize(query),
        aliasize(query).replace(" ", ""),
        query.replace(" ", "")
    ]
    seen_codes = set()

    for q in candidates_query:
        driver.get(_build_search_url(q))
        try:
            WebDriverWait(driver, wait_timeout).until(
                EC.any_of(
                    EC.url_contains("companyGradeInfo.do"),
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div.tbl_type01 table")),
                    EC.presence_of_element_located((By.CSS_SELECTOR, "table"))
                )
            )
        except Exception:
            pass

        # ìƒì„¸ë¡œ ë°”ë¡œ ì´ë™
        cmpcd = _parse_cmpcd_from_url(driver.current_url)
        if cmpcd:
            if cmpcd not in seen_codes:
                name, grade = fetch_company_and_grade_by_cmpcd(cmpcd, session)
                if grade:
                    seen_codes.add(cmpcd)
                    results.append({"ìš”ì²­ê²€ìƒ‰ì–´": query, "íšŒì‚¬ëª…": name, "cmpCd": cmpcd, "ë“±ê¸‰": grade})
            return results  # ìƒì„¸ë©´ 1ê±´ìœ¼ë¡œ ì¢…ë£Œ

        # ëª©ë¡(í‘œ) íŒŒì‹±
        html = driver.page_source
        cands = _extract_candidates_from_search_table(html, require_bond=require_bond_in_table)
        if not cands:
            time.sleep(table_wait_extra)  # ì¡°ê¸ˆ ë” ê¸°ë‹¤ë ¸ë‹¤ ì¬ì‹œë„
            html = driver.page_source
            cands = _extract_candidates_from_search_table(html, require_bond=require_bond_in_table)

        if cands:
            for cand in cands:
                code = cand["cmpCd"]
                if code in seen_codes:
                    continue
                name, grade = fetch_company_and_grade_by_cmpcd(code, session)
                if not grade:
                    continue
                seen_codes.add(code)
                results.append({"ìš”ì²­ê²€ìƒ‰ì–´": query, "íšŒì‚¬ëª…": name, "cmpCd": code, "ë“±ê¸‰": grade})
            return results

        # ë‹¤ìŒ ë³€í˜•ìœ¼ë¡œ ê³„ì†
    return results

# -------------------- ë“œë¼ì´ë²„ ê´€ë¦¬ --------------------
def _new_driver(driver_path: str) -> webdriver.Chrome:
    options = _stealth_options()
    driver = webdriver.Chrome(service=Service(driver_path), options=options)
    _post_launch_stealth(driver)
    return driver

# -------------------- ë³‘ë ¬ ì›Œì»¤ --------------------
def worker_process(batch: list[str], worker_id: int, driver_path: str) -> dict:
    """
    ìŠ¤ë ˆë“œ 1ê°œê°€ ë°°ì¹˜(ê¸°ì—… ë¦¬ìŠ¤íŠ¸)ë¥¼ ì²˜ë¦¬.
    - ì„¸ì…˜ ê¹¨ì§€ë©´ ë“œë¼ì´ë²„ ì¬ìƒì„± + 1íšŒ ì¬ì‹œë„
    """
    rows, skipped = [], []
    driver = _new_driver(driver_path)
    session = requests.Session()
    session.headers.update(HEADERS)

    try:
        for i, q in enumerate(batch, 1):
            retry_once = False
            while True:
                try:
                    found = search_and_collect_resilient(
                        driver, q, session,
                        wait_timeout=SEARCH_TIMEOUT,
                        table_wait_extra=0.8,
                        require_bond_in_table=True
                    )
                    if not found:
                        print(f"[W{worker_id}] ğŸ” ê²°ê³¼ ì—†ìŒ: {q}")
                        skipped.append((q, "ê²°ê³¼ ì—†ìŒ"))
                    else:
                        rows.extend(found)
                    break  # ì •ìƒ ì²˜ë¦¬ or ê²°ê³¼ ì—†ìŒ â†’ ë‹¤ìŒ íšŒì‚¬
                except WebDriverException as e:
                    msg = str(e).lower()
                    if not retry_once and ("invalid session id" in msg or "disconnected" in msg or "chrome not reachable" in msg):
                        print(f"[W{worker_id}] â™»ï¸ ë“œë¼ì´ë²„ ì¬ìƒì„± í›„ ì¬ì‹œë„: {q}")
                        try:
                            driver.quit()
                        except Exception:
                            pass
                        driver = _new_driver(driver_path)
                        retry_once = True
                        continue
                    else:
                        print(f"[W{worker_id}] â— ê²€ìƒ‰ ì‹¤íŒ¨: {q} -> {e}")
                        skipped.append((q, "ê²€ìƒ‰ ì‹¤íŒ¨"))
                        break
                except Exception as e:
                    print(f"[W{worker_id}] â— ê²€ìƒ‰ ì‹¤íŒ¨: {q} -> {e}")
                    skipped.append((q, "ê²€ìƒ‰ ì‹¤íŒ¨"))
                    break

            if i % 10 == 0:
                print(f"[W{worker_id}] ì§„í–‰ë¥ : {i}/{len(batch)}")
            time.sleep(0.25)  # ì„œë²„ ì˜ˆì˜
    finally:
        try:
            driver.quit()
        except Exception:
            pass

    return {"rows": rows, "skipped": skipped}

# -------------------- ìµœì¢… ë¦¬íŠ¸ë¼ì´(ì§ë ¬) --------------------
def retry_failed_serial(failed_names: list[str], driver_path: str) -> tuple[list[dict], list[tuple[str, str]]]:
    """
    1ë¼ìš´ë“œì—ì„œ 'ê²€ìƒ‰ ì‹¤íŒ¨/ê²°ê³¼ ì—†ìŒ'ì´ì—ˆë˜ ê¸°ì—…ì„
    - ë‹¨ì¼ ë“œë¼ì´ë²„ë¡œ ì§ë ¬ ì¬ê²€ìƒ‰
    - ëŒ€ê¸°ì‹œê°„ì„ ëŠ˜ë¦¬ê³ (require_bond_in_table=False) ëª©ë¡í‘œì—ì„œë„ cmpCdë§Œ ìˆìœ¼ë©´ ìƒì„¸ ì§„ì…
    """
    rows, skipped = [], []
    if not failed_names:
        return rows, skipped

    print(f"\nğŸ” ìµœì¢… ë¦¬íŠ¸ë¼ì´ ëŒ€ìƒ ìˆ˜: {len(failed_names)}")
    driver = _new_driver(driver_path)
    session = requests.Session()
    session.headers.update(HEADERS)

    try:
        for idx, q in enumerate(failed_names, 1):
            try:
                found = search_and_collect_resilient(
                    driver, q, session,
                    wait_timeout=FINAL_RETRY_TIMEOUT,
                    table_wait_extra=FINAL_RETRY_SLEEP,
                    require_bond_in_table=False  # âœ… í‘œì— 'ì±„ê¶Œ' ê°’ì´ ì—†ì–´ë„ cmpCdë§Œ ìˆìœ¼ë©´ ìƒì„¸ ì¬ì‹œë„
                )
                if not found:
                    print(f"[RETRY] ì—¬ì „íˆ ê²°ê³¼ ì—†ìŒ: {q}")
                    skipped.append((q, "ìµœì¢… ê²°ê³¼ ì—†ìŒ"))
                else:
                    rows.extend(found)
            except Exception as e:
                print(f"[RETRY] â— ì‹¤íŒ¨: {q} -> {e}")
                skipped.append((q, "ìµœì¢… ê²€ìƒ‰ ì‹¤íŒ¨"))

            if idx % 20 == 0:
                print(f"[RETRY] ì§„í–‰ë¥ : {idx}/{len(failed_names)}")
            time.sleep(0.35)
    finally:
        try:
            driver.quit()
        except Exception:
            pass

    return rows, skipped

# -------------------- ì…/ì¶œë ¥ & ë©”ì¸ --------------------
def load_companies_from_txt() -> list[str]:
    base_dir = os.path.dirname(__file__)
    path = os.path.join(base_dir, "..", "data", "input", "companies.txt")
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8-sig") as f:
        return [ln.strip() for ln in f if ln.strip()]

def save_dataframe(df: pd.DataFrame, suffix="parallel") -> str:
    base_dir = os.path.dirname(__file__)
    out_dir = os.path.join(base_dir, "..", "data", "output")
    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, f"ë‚˜ì´ìŠ¤_íšŒì‚¬ì±„ë“±ê¸‰_by_search_{suffix}_{datetime.now():%Y%m%d_%H%M}.csv")
    df.to_csv(out_path, encoding="utf-8-sig")
    return out_path

def chunk_by_size(lst: list, size: int) -> list[list]:
    return [lst[i:i+size] for i in range(0, len(lst), size)]

def main():
    companies = load_companies_from_txt()
    if not companies:
        print("âš  companies.txtê°€ ë¹„ì–´ìˆê±°ë‚˜ ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    print(f"ğŸ“‹ ëŒ€ìƒ ê²€ìƒ‰ì–´ ìˆ˜: {len(companies)}")

    driver_path = ChromeDriverManager().install()

    # ---- 1ë¼ìš´ë“œ: ë³‘ë ¬ ì²˜ë¦¬ ----
    if BATCH_SIZE_AUTO:
        size = max(1, math.ceil(len(companies) / MAX_WORKERS))
    else:
        size = 40
    batches = chunk_by_size(companies, size)
    print(f"ğŸ§µ ì›Œì»¤ ìˆ˜: {MAX_WORKERS}, ë°°ì¹˜ {len(batches)}ê°œ, ë°°ì¹˜ í¬ê¸° â‰ˆ {size}")

    rows_all, skipped_all = [], []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futs = {ex.submit(worker_process, batch, idx+1, driver_path): idx+1
                for idx, batch in enumerate(batches)}
        for fut in as_completed(futs):
            wid = futs[fut]
            try:
                res = fut.result()
                rows_all.extend(res["rows"])
                skipped_all.extend(res["skipped"])
                print(f"[W{wid}] âœ… ì™„ë£Œ: rows={len(res['rows'])}, skipped={len(res['skipped'])}")
            except Exception as e:
                print(f"[W{wid}] â— ì›Œì»¤ ì˜ˆì™¸: {e}")

    # ---- 2ë¼ìš´ë“œ: ìµœì¢… ë¦¬íŠ¸ë¼ì´ ----
    if FINAL_RETRY:
        # 1ë¼ìš´ë“œì—ì„œ 'ê²€ìƒ‰ ì‹¤íŒ¨' ë˜ëŠ” 'ê²°ê³¼ ì—†ìŒ' í•­ëª©ë§Œ ëŒ€ìƒìœ¼ë¡œ í•¨
        failed_names = [name for name, why in skipped_all if why in ("ê²€ìƒ‰ ì‹¤íŒ¨", "ê²°ê³¼ ì—†ìŒ")]
        failed_names = list(dict.fromkeys(failed_names))  # ì¤‘ë³µ ì œê±°, ìˆœì„œ ë³´ì¡´

        if failed_names:
            rows_retry, skipped_retry = retry_failed_serial(failed_names, driver_path)
            rows_all.extend(rows_retry)
            # ìµœì¢… ìŠ¤í‚µì— ëŒ€í•´ì„œëŠ” ì‚¬ìœ ë¥¼ 'ìµœì¢… ...'ìœ¼ë¡œ ìœ ì§€
            skipped_all = [(n, w) for (n, w) in skipped_all if n not in failed_names] + skipped_retry

    # ---- ê²°ê³¼ ì¶œë ¥/ì €ì¥ ----
    if not rows_all:
        print("âš  ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = (
        pd.DataFrame(rows_all)
        .drop_duplicates(subset=["cmpCd"])
        .set_index("íšŒì‚¬ëª…")
        .sort_index()
    )
    print(df.head(10))

    out_path = save_dataframe(df, suffix="parallel_retry")
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {out_path}")

    if skipped_all:
        print("\nâ€” ê±´ë„ˆë›´ ê²€ìƒ‰ì–´(ì‚¬ìœ ) â€”")
        for name, why in skipped_all[:80]:
            print(f"  â€¢ {name}: {why}")
        if len(skipped_all) > 80:
            print(f"  â€¦ì™¸ {len(skipped_all)-80}ê±´")

if __name__ == "__main__":
    main()
