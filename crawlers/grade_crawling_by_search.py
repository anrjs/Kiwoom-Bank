# -*- coding: utf-8 -*-
"""
NICE ì‹ ìš©í‰ê°€ - ê²€ìƒ‰ í›„ URLì—ì„œ cmpCd ì¶”ì¶œ â†’ ë“±ê¸‰ ìˆ˜ì§‘
- ê²€ìƒ‰ â†’ (ê°€ì¥ ìœ ì‚¬í•œ ê²°ê³¼) í´ë¦­ â†’ URLì˜ cmpCd íŒŒì‹±
- companyGradeInfo.do?cmpCd=... ìƒì„¸ í˜ì´ì§€ë¡œ ì§„ì…í•´ ì±„ê¶Œë“±ê¸‰ íŒŒì‹±
"""

import re
import time
import unicodedata
from pathlib import Path
from difflib import SequenceMatcher
from urllib.parse import urlparse, parse_qs

import requests
import pandas as pd
from bs4 import BeautifulSoup

# Selenium
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager


# ---------------- Basic config ----------------
BASE = "https://www.nicerating.com"
HOME = f"{BASE}/"
SEARCH_TIMEOUT = 20
REQUEST_TIMEOUT = 15
HEADLESS = True  # ë””ë²„ê¹…ì‹œ False

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/118.0.0.0 Safari/537.36"
    )
}


# ---------------- Name normalization / similarity ----------------
ALIASES = [
    (re.compile(r"\bì—ìŠ¤ì¼€ì´", re.I), "SK"),
    (re.compile(r"\bì—˜ì§€", re.I), "LG"),
    (re.compile(r"\bì”¨ì œì´", re.I), "CJ"),
    (re.compile(r"\bì¼€ì´í‹°\b", re.I), "KT"),
    (re.compile(r"\bì¼€ì´ë¹„\b", re.I), "KB"),
    (re.compile(r"\bì—”ì—ì´ì¹˜\b", re.I), "NH"),
    (re.compile(r"\bì—ì´ì¹˜ë””", re.I), "HD"),
    (re.compile(r"\bì—ì´ì¼€ì´\b", re.I), "AK"),
]
def aliasize(s: str) -> str:
    for pat, rep in ALIASES:
        s = pat.sub(rep, s)
    return re.sub(r"\s*\(ì£¼\)|ãˆœ", "", s).strip()

def normalize_text(s: str) -> str:
    if not s: return ""
    s = unicodedata.normalize("NFKC", s).strip().lower()
    s = re.sub(r"(ì£¼ì‹íšŒì‚¬|ãˆœ|\(ì£¼\)|ìœ í•œíšŒì‚¬|í™€ë”©ìŠ¤)", " ", s)
    s = re.sub(r"[\(\)\[\]\{\}]", " ", s)
    return re.sub(r"\s+", " ", s)

def name_similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, normalize_text(a), normalize_text(b)).ratio()


# ---------------- Detail page parser (ë‹¹ì‹ ì˜ ì›ë³¸ ë¡œì§ ìœ ì§€) ----------------
def _extract_name_from_tbl_type99(soup: BeautifulSoup) -> str | None:
    tbl = soup.select_one("div.tbl_type99 table")
    if not tbl: return None
    tbody = tbl.find("tbody")
    first_td = (tbody.find("td") if tbody else tbl.find("td"))
    return first_td.get_text(" ", strip=True) if first_td else None

def _extract_grade_primary_tbl1(soup: BeautifulSoup) -> str | None:
    table = soup.find("table", {"id": "tbl1"})
    if not table: return None
    tds = table.find_all("td", class_="cell_txt01")
    if not tds: return None
    return tds[0].get_text(strip=True) or None

# ë³´ê°•: 'íšŒì‚¬ì±„' í–‰ì—ì„œ í˜„ì¬ ë“±ê¸‰/ì „ë§
LONGTERM_GRADE_RE = re.compile(
    r"^(AAA|AA\+|AA|AA\-|A\+|A|A\-|BBB\+|BBB|BBB\-|BB\+|BB|BB\-|B\+|B|B\-|CCC|CC|C|D)$"
)
OUTLOOK_RE = re.compile(r"(ì•ˆì •ì |ê¸ì •ì |ë¶€ì •ì |ìœ ë™ì |Stable|Positive|Negative|Developing)", re.I)

def _extract_grade_from_major_table(soup: BeautifulSoup) -> str | None:
    for tr in soup.find_all("tr"):
        t = tr.get_text(" ", strip=True)
        if "íšŒì‚¬ì±„" in t:
            toks = t.replace("/", " ").split()
            grades = [x for x in toks if LONGTERM_GRADE_RE.match(x)]
            outs   = [x for x in toks if OUTLOOK_RE.search(x)]
            if grades:
                g = grades[-1]
                return f"{g} {outs[-1]}" if outs else g
    return None

def fetch_company_and_grade_by_cmpcd(cmpCd: str, session: requests.Session) -> tuple[str, str]:
    # ì‚¼ì„±ì „ì ìˆ˜ë™ ì ‘ì† ì‹œ ë³´ì´ë˜ íŒŒë¼ë¯¸í„° í¬í•¨ (êµ³ì´ ì—†ì–´ë„ ë¨)
    url = f"{BASE}/disclosure/companyGradeInfo.do?cmpCd={cmpCd}&deviceType=N&isPaidMember=false"
    resp = session.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    name = _extract_name_from_tbl_type99(soup)
    if not name:
        for sel in ["div.cont_title h3", "h3.tit", ".company_name", ".cmp_name", "div.title_area h3"]:
            el = soup.select_one(sel)
            if el: name = el.get_text(strip=True); break
    if not name:
        title_text = soup.title.get_text(strip=True) if soup.title else ""
        name = title_text.split("|")[0].split("-")[0].strip() or cmpCd

    grade = _extract_grade_primary_tbl1(soup) or _extract_grade_from_major_table(soup) or ""
    return name, grade


# ---------------- Selenium helpers ----------------
def _stealth_options() -> Options:
    opts = Options()
    if HEADLESS: opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--lang=ko-KR")
    opts.add_argument("--window-size=1280,2000")
    opts.add_argument(f"user-agent={HEADERS['User-Agent']}")
    # ë´‡ íŠ¹ì„± ì œê±°
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    return opts

def _post_launch_stealth(driver):
    try:
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {"source": 'Object.defineProperty(navigator, "webdriver", {get: () => undefined});'}
        )
    except Exception:
        pass

def _find_search_box(driver: webdriver.Chrome):
    sels = [
        "input#searchKeyword", "input#keyword",
        "input[name='searchKeyword']", "input[name='keyword']",
        "header input[type='text']", "form input[type='text']",
        "input[type='search']",
    ]
    for sel in sels:
        try:
            el = WebDriverWait(driver, 6).until(EC.presence_of_element_located((By.CSS_SELECTOR, sel)))
            if el.is_displayed(): return el
        except Exception:
            pass
    raise RuntimeError("ê²€ìƒ‰ ì…ë ¥ì°½ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

def _close_overlays(driver):
    for sel in ["button.close", ".btn_close", "a.close", "button[aria-label='ë‹«ê¸°']"]:
        try:
            for el in driver.find_elements(By.CSS_SELECTOR, sel):
                if el.is_displayed(): el.click()
        except Exception:
            pass

def _switch_to_new_tab_if_opened(driver, prev_handles):
    # ìƒˆ íƒ­ì´ ì—´ë ¸ìœ¼ë©´ ì „í™˜
    new_handles = driver.window_handles
    if len(new_handles) > len(prev_handles):
        for h in new_handles:
            if h not in prev_handles:
                driver.switch_to.window(h)
                return True
    return False

def _parse_cmpcd_from_url(url: str) -> str | None:
    # ?cmpCd=1326874&... ì—ì„œ cmpCdë§Œ ì¶”ì¶œ
    try:
        q = parse_qs(urlparse(url).query)
        if "cmpCd" in q and q["cmpCd"]:
            return q["cmpCd"][0]
    except Exception:
        pass
    m = re.search(r"cmpCd=(\d+)", url)
    return m.group(1) if m else None


def search_company_then_get_cmpcd_from_url(driver: webdriver.Chrome, company_name: str) -> str | None:
    """
    1) HOMEì—ì„œ ê²€ìƒ‰ â†’ ê²°ê³¼ ì¤‘ ê°€ì¥ ìœ ì‚¬í•œ í•­ëª© í´ë¦­
    2) í´ë¦­ í›„ ë¡œë“œëœ í˜ì´ì§€(or ìƒˆ íƒ­)ì˜ URLì—ì„œ cmpCd íŒŒì‹±
    """
    # ì›ë¬¸ â†’ ë³„ì¹­(ì—ìŠ¤ì¼€ì´â†’SK ë“±) ë‘ ë²ˆ ì‹œë„
    for attempt, query in enumerate([company_name, aliasize(company_name)], 1):
        driver.get(HOME)
        time.sleep(1.0)
        _close_overlays(driver)

        # ê²€ìƒ‰ ì…ë ¥
        box = _find_search_box(driver)
        box.clear(); box.send_keys(query); time.sleep(0.2)
        prev_handles = driver.window_handles[:]
        box.send_keys(Keys.ENTER)

        # ê²°ê³¼ í‘œì‹œ ëŒ€ê¸° (ë§í¬/onclick/iframe ë“±)
        try:
            WebDriverWait(driver, SEARCH_TIMEOUT).until(
                EC.any_of(
                    EC.presence_of_element_located((By.CSS_SELECTOR, '[onclick*="fn_cmpGradeInfo("]')),
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href*="companyGradeInfo.do"]')),
                    EC.presence_of_element_located((By.CSS_SELECTOR, "iframe")),
                    EC.url_contains("companyGradeInfo.do")
                )
            )
        except Exception:
            pass

        # ë§Œì•½ ì§ì ‘ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™í–ˆë‹¤ë©´ ë°”ë¡œ URL íŒŒì‹±
        cmpcd = _parse_cmpcd_from_url(driver.current_url)
        if cmpcd: return cmpcd

        # ìë™ì™„ì„±/ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹œë„
        if not cmpcd:
            try:
                box.send_keys(Keys.ARROW_DOWN); box.send_keys(Keys.ENTER)
                WebDriverWait(driver, 4).until(EC.url_contains("companyGradeInfo.do"))
                cmpcd = _parse_cmpcd_from_url(driver.current_url)
                if cmpcd: return cmpcd
            except Exception:
                pass

        # ìˆ˜ë™ìœ¼ë¡œ ê²°ê³¼ í•­ëª© í´ë¦­ (ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ ê²ƒ)
        candidates = []
        # a[href*=companyGradeInfo]
        for a in driver.find_elements(By.CSS_SELECTOR, 'a[href*="companyGradeInfo.do"]'):
            try:
                txt = (a.text or a.get_attribute("title") or "").strip()
                sc = name_similarity(company_name, txt)
                candidates.append((sc, a))
            except Exception:
                pass
        # onclick=fn_cmpGradeInfo(...)
        for el in driver.find_elements(By.CSS_SELECTOR, '[onclick*="fn_cmpGradeInfo("]'):
            try:
                txt = (el.text or el.get_attribute("title") or "").strip()
                sc = name_similarity(company_name, txt)
                candidates.append((sc, el))
            except Exception:
                pass

        # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ í´ë¦­ ì‹œë„
        candidates.sort(key=lambda x: x[0], reverse=True)
        for sc, el in candidates[:5]:
            try:
                if sc < (0.35 if attempt == 2 else 0.5):
                    continue
                prev = driver.window_handles[:]
                el.click()
                time.sleep(1.0)
                _switch_to_new_tab_if_opened(driver, prev)
                WebDriverWait(driver, 6).until(EC.url_contains("companyGradeInfo.do"))
                cmpcd = _parse_cmpcd_from_url(driver.current_url)
                if cmpcd: return cmpcd
            except Exception:
                # ë‹¤ë¥¸ í›„ë³´ë¡œ ê³„ì† ì‹œë„
                continue

        # ì—¬ê¸°ê¹Œì§€ ì‹¤íŒ¨ë©´ alias ì‹œë„ë¡œ ë„˜ì–´ê°
    return None


# ---------------- I/O & pipeline ----------------
def load_companies_from_txt(path="../data/input/companies.txt") -> list[str]:
    p = Path(path)
    if not p.exists(): return []
    return [ln.strip() for ln in p.read_text(encoding="utf-8-sig").splitlines() if ln.strip()]

def main():
    companies = load_companies_from_txt("../data/input/companies.txt")
    print(f"ğŸ“‹ ëŒ€ìƒ ê¸°ì—… ìˆ˜: {len(companies)}")

    options = _stealth_options()
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    _post_launch_stealth(driver)

    session = requests.Session()
    session.headers.update(HEADERS)

    rows, skipped = [], []

    try:
        for i, corp in enumerate(companies, 1):
            try:
                cmpcd = search_company_then_get_cmpcd_from_url(driver, corp)
            except Exception as e:
                print(f"â— ê²€ìƒ‰ ì‹¤íŒ¨: {corp} -> {e}")
                skipped.append((corp, "ê²€ìƒ‰ ì‹¤íŒ¨")); continue

            if not cmpcd:
                print(f"ğŸ” ê²€ìƒ‰ê²°ê³¼/URLì— cmpCd ì—†ìŒ: {corp}")
                skipped.append((corp, "cmpCd ì—†ìŒ")); continue

            # ë™ì¼ ì„¸ì…˜ ìœ ì§€(í•„ìš” ì‹œ)
            for c in driver.get_cookies():
                session.cookies.set(c["name"], c["value"], domain=c.get("domain") or "www.nicerating.com")

            try:
                name, grade = fetch_company_and_grade_by_cmpcd(cmpcd, session)
            except Exception as e:
                print(f"â— ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {corp} (cmpCd={cmpcd}) -> {e}")
                skipped.append((corp, "ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨")); continue

            if not grade:
                print(f"â­ ë“±ê¸‰ ì—†ìŒ: {name} (ìš”ì²­: {corp}, cmpCd={cmpcd})")
                skipped.append((corp, "ë“±ê¸‰ ì—†ìŒ")); continue

            rows.append({"ìš”ì²­ê¸°ì—…ëª…": corp, "íšŒì‚¬ëª…": name, "cmpCd": cmpcd, "ë“±ê¸‰": grade})

            if i % 10 == 0:
                print(f"  ì§„í–‰ë¥ : {i}/{len(companies)}")
            time.sleep(0.3)
    finally:
        driver.quit()

    if not rows:
        print("âš  ë“±ê¸‰ì„ ì¶”ì¶œí•œ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = pd.DataFrame(rows).set_index("íšŒì‚¬ëª…").sort_index()
    ts = pd.Timestamp.now().strftime("%Y%m%d_%H%M")
    out = f"ë‚˜ì´ìŠ¤_íšŒì‚¬ì±„ë“±ê¸‰_by_url_{ts}.csv"
    df.to_csv(out, encoding="utf-8-sig")
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {out}")
    print(df.head(10))

    if skipped:
        print("\nâ€” ê±´ë„ˆë›´ ê¸°ì—…(ì‚¬ìœ ) â€”")
        for corp, why in skipped:
            print(f"  â€¢ {corp}: {why}")

if __name__ == "__main__":
    main()
