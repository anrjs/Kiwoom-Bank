# src/kiwoom_finance/batch.py
from __future__ import annotations

import os
import time
import traceback
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, TimeoutError
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional
import hashlib
import json
import re
import sys

import pandas as pd
from tqdm import tqdm

# ë‚´ë¶€ ëª¨ë“ˆ
from .dart_client import extract_fs, find_corp, init_dart, IdentifierType
from .preprocess import preprocess_all
from .metrics import compute_metrics_df_flat_kor

# dart_fss íŠ¹ì • ì˜ˆì™¸(ì—°ê²°ì¬ë¬´ ë¯¸ë°œê²¬) ì‹ë³„ìš©
try:
    from dart_fss.errors.errors import NotFoundConsolidated  # type: ignore
except Exception:  # dart_fss ë¯¸ë¡œë”© í™˜ê²½ ëŒ€ë¹„
    NotFoundConsolidated = tuple()  # noqa: N816


# -----------------------
# ê¸°ë³¸ ì‚°ì¶œ ì»¬ëŸ¼(ì¡´ì¬ ì‹œì—ë§Œ ì„ íƒ)
# -----------------------
DEFAULT_COLS = [
    "debt_ratio", "equity_ratio", "debt_dependency_ratio",
    "current_ratio", "quick_ratio", "interest_coverage_ratio",
    "ebitda_to_total_debt", "cfo_to_total_debt", "free_cash_flow",
    "operating_margin", "roa", "roe", "net_profit_margin",
    "total_asset_turnover", "accounts_receivable_turnover", "inventory_turnover",
    "sales_growth_rate", "operating_income_growth_rate", "total_asset_growth_rate",
]


# ======================================
# ìºì‹œ/ì¶œë ¥ ìœ í‹¸ (ì•ˆì „ ë™ì‘ ë³´ì¥ìš© í—¬í¼)
# ======================================

def _resolve_output_dir_safely(output_dir: str | Path) -> Path:
    """
    ì•ˆì „í•˜ê²Œ ì¶œë ¥ ë””ë ‰í„°ë¦¬ë¥¼ ë°˜í™˜.
    - ë§Œì•½ ê°™ì€ ê²½ë¡œì— 'íŒŒì¼'ì´ ì¡´ì¬í•˜ë©´ '_dir'ì„ ë§ë¶™ì¸ ë””ë ‰í„°ë¦¬ë¡œ ìš°íšŒ ìƒì„±
    """
    p = Path(output_dir)
    if p.exists() and p.is_file():
        p = p.with_name(p.name + "_dir")
    p.mkdir(parents=True, exist_ok=True)
    return p


def _load_cached_csv(code: str, output_dir_path: Path) -> pd.DataFrame | None:
    """
    ê³¼ê±° ì €ì¥í•œ CSV(ì½”ë“œë³„ 1ê°œ)ë¥¼ ì½ì–´ì„œ DataFrame ë°˜í™˜. ì—†ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ None.
    """
    csv_path = output_dir_path / f"{code}.csv"
    if not csv_path.exists():
        return None
    try:
        df = pd.read_csv(csv_path, index_col=0, encoding="utf-8-sig")
        return df
    except Exception:
        return None


def _resolve_cache_dir(cache_dir: str | Path | None) -> Path | None:
    """
    í”¼í´ ìºì‹œ ë£¨íŠ¸ ê²½ë¡œ ë°˜í™˜. Noneì´ë©´ ë¹„í™œì„±.
    """
    if cache_dir is None:
        return None
    root = Path(cache_dir)
    root.mkdir(parents=True, exist_ok=True)
    return root


def _build_cache_key(
    *,
    code: str,
    bgn_de: str,
    report_tp: str,
    separate: bool,
    latest_only: bool,
    percent_format: bool,
) -> str:
    """
    íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ë°”íƒ•ìœ¼ë¡œ ì•ˆì •ì ì¸ ìºì‹œ í‚¤ ìƒì„±
    """
    payload = {
        "code": code,
        "bgn_de": bgn_de,
        "report_tp": report_tp,
        "separate": separate,
        "latest_only": latest_only,
        "percent_format": percent_format,
        # í–¥í›„ ì˜µì…˜ ì¶”ê°€ ëŒ€ë¹„
    }
    raw = json.dumps(payload, sort_keys=True, ensure_ascii=False).encode("utf-8")
    return hashlib.sha256(raw).hexdigest()


def _load_cached_frame(cache_root: Path, cache_key: str, ttl_seconds: float | None) -> pd.DataFrame | None:
    """
    ìºì‹œ íŒŒì¼ ì½ê¸°. TTL(ì´ˆ) ë‚´ë©´ ë°˜í™˜, ì´ˆê³¼ë©´ None.
    """
    pkl_path = cache_root / f"{cache_key}.pkl"
    if not pkl_path.exists():
        return None
    try:
        if ttl_seconds is not None and ttl_seconds > 0:
            age = time.time() - os.stat(pkl_path).st_mtime
            if age > ttl_seconds:
                return None
        return pd.read_pickle(pkl_path)
    except Exception:
        return None


def _store_cached_frame(cache_root: Path, cache_key: str, df: pd.DataFrame) -> None:
    """
    ìºì‹œ íŒŒì¼ ì €ì¥
    """
    pkl_path = cache_root / f"{cache_key}.pkl"
    try:
        df.to_pickle(pkl_path)
    except Exception:
        # ìºì‹œ ì‹¤íŒ¨ëŠ” ì¹˜ëª… ì•„ë‹˜
        pass


# -----------------------
# ì›Œì»¤/í’ˆì§ˆ/í›„ë³´ ì»¬ëŸ¼ ì„ íƒ
# -----------------------

@dataclass
class WorkerConfig:
    bgn_de: str
    report_tp: str
    separate: bool
    latest_only: bool
    percent_format: bool
    api_key: Optional[str] = None
    retries: int = 3
    throttle_sec: float = 1.2  # DART ë¶€í•˜ ë°©ì§€ìš© sleep (1.0 -> 1.2)


@dataclass
class LookupTarget:
    identifier: str
    stock_code: str
    corp_name: str | None = None

    @property
    def label(self) -> str:
        base = self.corp_name or self.identifier
        return f"{base}({self.stock_code})"


def _normalize_stock_code(raw: str | int | None) -> str | None:
    if raw is None:
        return None
    code = str(raw).strip()
    if not code:
        return None
    if code.isdigit() and len(code) < 6:
        code = code.zfill(6)
    return code


def _select_existing_cols(df):
    keep = [c for c in DEFAULT_COLS if c in df.columns]
    return df[keep].copy() if keep else df.copy()   # â† ì—†ìœ¼ë©´ ì›ë³¸ ìœ ì§€


def _try_extract_with_fallback(corp, cfg: WorkerConfig):
    """
    1ì°¨: cfg.separate ì„¤ì •ëŒ€ë¡œ ì‹œë„
    2ì°¨: NotFoundConsolidated ì‹œ ë°˜ëŒ€ separateë¡œ í´ë°± ì‹œë„
    """
    try:
        return extract_fs(
            corp,
            bgn_de=cfg.bgn_de,
            report_tp=cfg.report_tp,
            separate=cfg.separate,
        )
    except Exception as e:
        # ì—°ê²°/ë³„ë„ ë¯¸ë°œê²¬ ì‹œ í´ë°±
        is_nfc = False
        if NotFoundConsolidated and isinstance(e, NotFoundConsolidated):  # ì •ì‹ íŒë³„
            is_nfc = True
        elif "NotFoundConsolidated" in f"{type(e)} {e}":  # ë¬¸ìì—´ íŒë³„(ë³´ì¡°)
            is_nfc = True
        if is_nfc:
            # ë°˜ëŒ€ separateë¡œ ì¬ì‹œë„
            return extract_fs(
                corp,
                bgn_de=cfg.bgn_de,
                report_tp=cfg.report_tp,
                separate=not cfg.separate,
            )
        raise


def _quality_ok(df: pd.DataFrame, nan_ratio_limit: float, min_non_null: int) -> bool:
    """
    ê²°ê³¼ í’ˆì§ˆ íŒì •:
    - NaN ë¹„ìœ¨ì´ í—ˆìš©ì¹˜ ì´í•˜
    - ë‹¨ì¼ í–‰ ê¸°ì¤€ ì±„ì›Œì§„(ìˆ«ì) ì§€í‘œ ê°œìˆ˜ â‰¥ min_non_null
    """
    if df is None or df.empty:
        return False

    # ìˆ«ìí˜• í›„ë³´ ì»¬ëŸ¼ ì„ ì •
    cand: list[str] = []
    for c in df.columns:
        if c == "stock_code":
            continue
        ser = pd.to_numeric(df[c], errors="coerce")
        if ser.notna().any():
            cand.append(c)

    if not cand:
        return False

    sub = df[cand].apply(pd.to_numeric, errors="coerce")
    non_null_max = int(sub.notna().sum(axis=1).max())
    nan_ratio = 1.0 - (non_null_max / len(cand))
    return (nan_ratio <= nan_ratio_limit) and (non_null_max >= min_non_null)


# ============
# ì›Œì»¤ (ë‹¨ì¼ ì¢…ëª©)
# ============
def _run_worker(code: str, cfg: WorkerConfig, identifier: str | None = None) -> pd.DataFrame | None:
    """
    ê°œë³„ ì¢…ëª© ì²˜ë¦¬:
    - (ë©€í‹°í”„ë¡œì„¸ìŠ¤ í˜¸í™˜) ì›Œì»¤ ë‚´ë¶€ì—ì„œ DART ì´ˆê¸°í™”
    - corp ì¡°íšŒ â†’ filings ì¶”ì¶œ(ì—°ê²°â†’ë³„ë„ í´ë°±) â†’ ì „ì²˜ë¦¬ â†’ ì§€í‘œ ì‚°ì¶œ â†’ ì»¬ëŸ¼ í•„í„°
    - latest_onlyë©´ ìµœì‹  1ê°œë§Œ
    ì‹¤íŒ¨ ì‹œ None
    """
    # í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œ ì–´ë””ì„œë“  ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”
    init_dart(cfg.api_key)

    label = identifier or code

    for attempt in range(1, cfg.retries + 1):
        try:
            # ë¶€í•˜ ë°©ì§€
            time.sleep(cfg.throttle_sec)

            corp = find_corp(code, by="code")
            if corp is None:
                raise ValueError(f"corp not found for {label}")

            corp_code = _normalize_stock_code(getattr(corp, "stock_code", None)) or code

            # FS ì¶”ì¶œ(ì—°ê²°â†’ë³„ë„ í´ë°± ë‚´ì¥)
            fs = _try_extract_with_fallback(corp, cfg)

            # í‘œì¤€í™”ëœ 4í‘œ ìƒì„±
            bs, is_, cis, cf = preprocess_all(fs)

            # ì§€í‘œ ê³„ì‚°
            df = compute_metrics_df_flat_kor(bs, is_, cis, cf, key_cols=None)

            # ìš”ì²­ëœ ì»¬ëŸ¼ë§Œ ì¶”ë¦¼(ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ)
            df = _select_existing_cols(df)

            if df.empty:
                tqdm.write(f"âš ï¸ [{label}] ë°ì´í„° ì—†ìŒ (ë¹ˆ DataFrame)")
                return None

            # ìµœì‹  1ê°œë§Œ
            if cfg.latest_only:
                df = df.sort_index(ascending=False).iloc[[0]]
                df.index = [corp_code]
            else:
                df.index = [f"{corp_code}_{i}" for i in range(len(df))]

            df.index.name = "stock_code"
            return df

        except Exception as e:
            tqdm.write(
                f"âš ï¸ [{label}] ì‹œë„ {attempt}/{cfg.retries} ì‹¤íŒ¨: {type(e).__name__}: {e}"
            )
            traceback.print_exc(limit=1)  # ìŠ¤íƒ ì¶”ì  ê³¼ë‹¤ ì¶œë ¥ ë°©ì§€
            time.sleep(0.9 * attempt)     # ì§€ìˆ˜ ë°±ì˜¤í”„(ì†Œí­ ìƒí–¥)

    tqdm.write(f"âŒ [{label}] {cfg.retries}íšŒ ì‹¤íŒ¨ í›„ ê±´ë„ˆëœ€.")
    return None


# =============
# ë©”ì¸ ì—”íŠ¸ë¦¬
# =============
def get_metrics_for_codes(
    codes: List[str],
    bgn_de: str = "20210101",
    report_tp: str = "annual",
    separate: bool = True,
    latest_only: bool = False,
    percent_format: bool = False,  # í˜„ì¬ ìˆ«ìí˜• ìœ ì§€ ê¸°ë³¸
    identifier_type: IdentifierType = "auto",
    api_key: Optional[str] = None,
    max_workers: int = 6,
    save_each: bool = False,
    output_dir: str = "artifacts/by_stock",
    # â”€â”€ ìƒˆ ì˜µì…˜: í’ˆì§ˆ/íƒ€ì„ì•„ì›ƒ/ì‹¤í–‰ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    per_code_timeout_sec: int | None = 150,   # ì¢…ëª©ë‹¹ í•˜ë“œ íƒ€ì„ì•„ì›ƒ(ì´ˆ). None/0ì´ë©´ ë¯¸ì ìš©
    skip_nan_heavy: bool = True,              # NaN ê³¼ë‹¤ ê²°ê³¼ ìŠ¤í‚µ
    nan_ratio_limit: float = 0.60,            # NaN â‰¤ 60%
    min_non_null: int = 5,                    # ìµœì†Œ ì±„ì›Œì§„ ì§€í‘œ 5ê°œ
    prefer_process: bool = False,             # ğŸ”§ Windows ê¸°ë³¸: ThreadPool (ProcessPoolì€ __main__ ê°€ë“œ í•„ìš”)
    # â”€â”€ í”¼í´ ìºì‹œ ì˜µì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cache_dir: str | Path | None = None,
    cache_ttl: float | int | None = None,
    force_refresh: bool = False,
) -> pd.DataFrame:
    """
    ë³µìˆ˜ ì¢…ëª© ì¬ë¬´ì§€í‘œë¥¼ ë³‘ë ¬ ìˆ˜ì§‘
    - CSV ì €ì¥ ìºì‹œ(save_each+output_dir) + í”¼í´ ìºì‹œ(cache_dir) í˜¼ìš© ì§€ì›
    - í’ˆì§ˆ í•„í„°ë¡œ NaN ê³¼ë‹¤ ì¢…ëª© ìŠ¤í‚µ ê°€ëŠ¥
    - ì¢…ëª©ë³„ í•˜ë“œ íƒ€ì„ì•„ì›ƒ
    - Windows ê¸°ë³¸ ThreadPool ì‚¬ìš©(ë©€í‹°í”„ë¡œì„¸ìŠ¤ëŠ” __main__ ë³´í˜¸ í•„ìš”)
    - identifier_type="auto"(ê¸°ë³¸)ë©´ ì¢…ëª©ëª…ì„ ìš°ì„  ê²€ìƒ‰ í›„ ì¢…ëª©ì½”ë“œë¥¼ ì‹œë„
    """
    if not codes:
        return pd.DataFrame(columns=DEFAULT_COLS)

    # ì´ë¦„ ê²€ìƒ‰ì„ ìœ„í•´ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë„ ì´ˆê¸°í™”(api_key ìš°ì„  ì ìš©)
    init_dart(api_key)

    # ğŸ”§ ì•ˆì „í•œ ë””ë ‰í„°ë¦¬ ìƒì„± (artifactsê°€ íŒŒì¼ì´ì–´ë„ ìš°íšŒ)
    output_dir_path = _resolve_output_dir_safely(output_dir)

    frames: list[pd.DataFrame] = []
    failed: list[tuple[str, str | None, str]] = []  # (identifier, stock_code, reason)
    skipped: list[tuple[str, str | None, str]] = []

    cfg = WorkerConfig(
        bgn_de=bgn_de,
        report_tp=report_tp,
        separate=separate,
        latest_only=latest_only,
        percent_format=percent_format,
        api_key=api_key,
        retries=3,
        throttle_sec=1.2,
    )

    # 0) ì…ë ¥ ì‹ë³„ì â†’ ì‹¤ì œ ì¢…ëª©ì½”ë“œ ë§¤í•‘
    targets: list[LookupTarget] = []
    for raw in codes:
        if raw is None:
            continue
        identifier = str(raw).strip()
        if not identifier:
            continue

        corp = find_corp(identifier, by=identifier_type)
        if corp is None:
            failed.append((identifier, None, "not_found"))
            tqdm.write(f"âŒ [{identifier}] ì¢…ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        stock_code = _normalize_stock_code(getattr(corp, "stock_code", None))
        if not stock_code:
            failed.append((identifier, None, "no_stock_code"))
            tqdm.write(f"âŒ [{identifier}] ì¢…ëª©ì½”ë“œë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        corp_name = getattr(corp, "corp_name", None)
        if corp_name is not None:
            corp_name = str(corp_name).strip() or None

        targets.append(LookupTarget(identifier=identifier, stock_code=stock_code, corp_name=corp_name))

    # ===== í”¼í´ ìºì‹œ ìš°ì„  ì¡°íšŒ =====
    cache_root = _resolve_cache_dir(cache_dir)
    ttl_seconds = float(cache_ttl) if cache_ttl is not None else None

    # ì œì¶œ ëŒ€ìƒ ê²°ì •: CSV ìºì‹œ â†’ í”¼í´ ìºì‹œ â†’ API
    submit_targets: list[LookupTarget] = []

    # 1) CSV ìºì‹œ ì²´í¬ (í•˜ìœ„ í˜¸í™˜)
    if save_each:
        for target in targets:
            csv_df = _load_cached_csv(target.stock_code, output_dir_path)
            if csv_df is not None:
                if (not skip_nan_heavy) or _quality_ok(csv_df, nan_ratio_limit, min_non_null):
                    frames.append(csv_df)
                    tqdm.write(f"âœ… [CSV ìºì‹œ ì‚¬ìš©] {target.label}")
                    continue
                else:
                    tqdm.write(f"âš ï¸ [CSV ìºì‹œ í’ˆì§ˆë¶ˆëŸ‰] {target.label} â†’ ì¬ì‹œë„ ì˜ˆì •")
            submit_targets.append(target)
    else:
        submit_targets = list(targets)

    # 2) í”¼í´ ìºì‹œ ì²´í¬
    frames_ordered: list[pd.DataFrame | None] = [None] * len(submit_targets)
    cache_keys: dict[int, str] = {}
    targets_to_fetch: list[tuple[int, LookupTarget]] = []

    if cache_root is not None:
        for idx, target in enumerate(submit_targets):
            ck = _build_cache_key(
                code=target.stock_code,
                bgn_de=bgn_de,
                report_tp=report_tp,
                separate=separate,
                latest_only=latest_only,
                percent_format=percent_format,
            )
            cache_keys[idx] = ck
            if not force_refresh:
                cached_df = _load_cached_frame(cache_root, ck, ttl_seconds)
                if cached_df is not None:
                    if (not skip_nan_heavy) or _quality_ok(cached_df, nan_ratio_limit, min_non_null):
                        frames_ordered[idx] = cached_df
                        tqdm.write(f"âœ… [í”¼í´ ìºì‹œ íˆíŠ¸] {target.label}")
                        continue
                    else:
                        tqdm.write(f"âš ï¸ [í”¼í´ ìºì‹œ í’ˆì§ˆë¶ˆëŸ‰] {target.label} â†’ ì¬ì‹œë„ ì˜ˆì •")
            targets_to_fetch.append((idx, target))
    else:
        targets_to_fetch = list(enumerate(submit_targets))

    # 3) API ë³‘ë ¬ ì²˜ë¦¬
    def _submit_and_collect(to_fetch: list[tuple[int, LookupTarget]]):
        if not to_fetch:
            return

        Executor = ProcessPoolExecutor if (prefer_process and sys.platform != "win32") else ThreadPoolExecutor

        with Executor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(_run_worker, target.stock_code, cfg, target.identifier): (idx, target)
                for idx, target in to_fetch
            }
            pbar = tqdm(total=len(futures), desc="ğŸ“Š ì¢…ëª© ì²˜ë¦¬ ì¤‘", ncols=100, dynamic_ncols=False)
            for future in as_completed(futures):
                idx, target = futures[future]
                try:
                    if per_code_timeout_sec and per_code_timeout_sec > 0:
                        df = future.result(timeout=per_code_timeout_sec)
                    else:
                        df = future.result()

                    if df is not None and not df.empty:
                        # í’ˆì§ˆ í•„í„°
                        if skip_nan_heavy and not _quality_ok(df, nan_ratio_limit, min_non_null):
                            skipped.append((target.identifier, target.stock_code, "nan_heavy"))
                            tqdm.write(f"â­ï¸  [{target.label}] NaN ê³¼ë‹¤ë¡œ ìŠ¤í‚µ")
                        else:
                            frames_ordered[idx] = df
                            # í”¼í´ ìºì‹œ ì €ì¥
                            if cache_root is not None:
                                ck = cache_keys.get(idx)
                                if ck:
                                    _store_cached_frame(cache_root, ck, df)
                            # CSV ìºì‹œ ì €ì¥(ì˜µì…˜)
                            if save_each:
                                out_path = output_dir_path / f"{target.stock_code}.csv"
                                out_path.parent.mkdir(parents=True, exist_ok=True)
                                df.to_csv(out_path, encoding="utf-8-sig")
                    else:
                        failed.append((target.identifier, target.stock_code, "empty"))
                except TimeoutError:
                    failed.append((target.identifier, target.stock_code, "timeout"))
                    tqdm.write(f"â±ï¸  [{target.label}] íƒ€ì„ì•„ì›ƒ({per_code_timeout_sec}s)ìœ¼ë¡œ ì‹¤íŒ¨ ì²˜ë¦¬")
                except Exception as e:
                    tqdm.write(f"âŒ [{target.label}] ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e}")
                    failed.append((target.identifier, target.stock_code, type(e).__name__))
                finally:
                    pbar.update(1)
            pbar.close()

    _submit_and_collect(targets_to_fetch)

    # ì‹¤íŒ¨/ìŠ¤í‚µ ëª©ë¡ ì €ì¥
    report_dir = _resolve_output_dir_safely("artifacts")
    if failed:
        fail_path = report_dir / "failed_codes.csv"
        fail_df = pd.DataFrame(failed, columns=["identifier", "stock_code", "reason"])
        fail_df["failed_code"] = fail_df["stock_code"].fillna(fail_df["identifier"])
        fail_df.to_csv(fail_path, index=False, encoding="utf-8-sig")
        print(f"\nâš ï¸ ì‹¤íŒ¨í•œ ì¢…ëª© {len(fail_df)}ê°œ â†’ {fail_path} ì €ì¥ë¨")
    if skipped:
        skip_path = report_dir / "skipped_codes.csv"
        skip_df = pd.DataFrame(skipped, columns=["identifier", "stock_code", "reason"])
        skip_df.to_csv(skip_path, index=False, encoding="utf-8-sig")
        print(f"â„¹ï¸ ìŠ¤í‚µëœ ì¢…ëª© {len(skip_df)}ê°œ â†’ {skip_path} ì €ì¥ë¨")

    # ê²°ê³¼ ë³‘í•©
    from_cache = [f for f in frames_ordered if f is not None]
    frames.extend(from_cache)

    if not frames:
        print("â— ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=DEFAULT_COLS)

    result = pd.concat(frames)
    print(f"\nâœ… ì™„ë£Œ! ì´ {len(result)}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ.")
    return result
