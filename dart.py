# test_metrics.py (repo ë£¨íŠ¸ì—ì„œ ì‹¤í–‰)
from __future__ import annotations

import os
import sys
import signal
import time
from datetime import datetime
from pathlib import Path
from typing import Iterable, List, Set, Optional

import pandas as pd
import warnings

# â”€â”€ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = Path(__file__).resolve().parent
SRC_DIR = BASE_DIR / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

# â”€â”€ .env ë¡œë“œ(ìˆìœ¼ë©´) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_dotenv_if_possible():
    try:
        from dotenv import load_dotenv, find_dotenv
        path = find_dotenv(usecwd=True)
        if path:
            load_dotenv(path)
            print(f"ğŸ§© .env ë¡œë“œ: {path}")
    except Exception:
        pass

_load_dotenv_if_possible()

# â”€â”€ ë‚´ë¶€ import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from kiwoom_finance.batch import get_metrics_for_codes

# â”€â”€ ê²½ê³  ì–µì œ(ì„ íƒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
warnings.filterwarnings("ignore", category=UserWarning, module="dart_fss")
warnings.filterwarnings("ignore", category=RuntimeWarning, module="dart_fss")

# â”€â”€ íƒ€ê¹ƒ ì¢…ëª©(ì›ë³¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CODES = ["005930"]

# â”€â”€ ìœ í‹¸: ì½”ë“œ ì •ì œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_codes(codes: Iterable[str]) -> List[str]:
    codes = [str(c) for c in codes]
    codes = [c for c in codes if c.isdigit() and len(c) <= 6]
    codes = [c.zfill(6) for c in codes]
    return list(dict.fromkeys(codes))  # ì¤‘ë³µ ì œê±°(ìˆœì„œ ë³´ì¡´)

# â”€â”€ ìºì‹œ/ì¶œë ¥ ê²½ë¡œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUTPUT_DIR = BASE_DIR / "artifacts" / "by_stock"  # per-stock CSV ìºì‹œ í´ë”
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

PKL_CACHE_DIR = BASE_DIR / "artifacts" / ".pklcache"
PKL_CACHE_DIR.mkdir(parents=True, exist_ok=True)

RESULT_CSV = BASE_DIR / "metrics_result.csv"
FAILED_CSV = BASE_DIR / "artifacts" / "failed_codes.csv"
SKIPPED_CSV = BASE_DIR / "artifacts" / "skipped_codes.csv"
SNAP_DIR = BASE_DIR / "artifacts" / "snapshots"
SNAP_DIR.mkdir(parents=True, exist_ok=True)

INTEGRITY_CSV = BASE_DIR / "artifacts" / "metrics_integrity_report.csv"

# â”€â”€ DART í‚¤: .env ìš°ì„ , ì—†ìœ¼ë©´ ë°±ì—… í‚¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API_KEY = os.getenv("DART_API_KEY", "").strip() or "YOUR_BACKUP_KEY"

# â”€â”€ ì§„í–‰ ì¤‘ ê°•ì œ ì¢…ë£Œ ëŒ€ë¹„: SIGINT/SIGTERM í•¸ë“¤ëŸ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_stop_requested = False
def _handle_stop(signum, frame):
    global _stop_requested
    _stop_requested = True
    print("\nğŸ›‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€. í˜„ì¬ ë°°ì¹˜ê°€ ëë‚˜ë©´ ì•ˆì „í•˜ê²Œ ì €ì¥í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤...")

for _sig in ("SIGINT", "SIGTERM"):
    if hasattr(signal, _sig):
        signal.signal(getattr(signal, _sig), _handle_stop)

# â”€â”€ íŒŒì¼ ê¸°ë°˜: ì™„ë£Œ/ë¯¸ì™„ë£Œ íŒë‹¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def list_done_codes_from_csv_cache(out_dir: Path) -> Set[str]:
    """artifacts/by_stock/*.csv íŒŒì¼ëª… ê¸°ì¤€ ì™„ë£Œ ì¢…ëª© ì„¸íŠ¸"""
    done = set()
    for p in out_dir.glob("*.csv"):
        code = p.stem
        if code.isdigit():
            done.add(code.zfill(6))
    return done

def compute_todo_codes(all_codes: List[str], out_dir: Path) -> List[str]:
    done = list_done_codes_from_csv_cache(out_dir)
    return [c for c in all_codes if c not in done]

# â”€â”€ ë³‘í•© ìœ í‹¸: ìºì‹œ CSV â†’ í•˜ë‚˜ì˜ í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def merge_by_stock_cache(out_dir: Path) -> pd.DataFrame:
    files = list(out_dir.glob("*.csv"))
    if not files:
        return pd.DataFrame()

    dfs = []
    for fp in files:
        try:
            df = pd.read_csv(fp, encoding="utf-8-sig")
            # ë³´ì¡°: stock_code ì—†ìœ¼ë©´ íŒŒì¼ëª…ìœ¼ë¡œ ë¶€ì—¬
            if "stock_code" not in df.columns:
                df["stock_code"] = fp.stem.zfill(6)
            else:
                # ë¬¸ìì—´ë¡œ ì •ê·œí™”
                df["stock_code"] = df["stock_code"].astype(str).str[:6].str.zfill(6)
            dfs.append(df)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {fp.name} ({e})")

    if not dfs:
        return pd.DataFrame()

    merged = pd.concat(dfs, ignore_index=True)

    # ê°™ì€ stock_code ì—¬ëŸ¬ í–‰ â†’ non-NaN ë¹„ì¤‘ ë†’ì€ í–‰ ìš°ì„ 
    def _non_na_score(row: pd.Series) -> int:
        return row.notna().sum()

    merged["_score"] = merged.apply(_non_na_score, axis=1)
    merged.sort_values(["stock_code", "_score"], ascending=[True, False], inplace=True)
    merged = merged.drop_duplicates(subset=["stock_code"], keep="first")
    merged = merged.drop(columns=["_score"], errors="ignore")
    merged.set_index("stock_code", inplace=True)
    return merged

def save_snapshot(df: pd.DataFrame, tag: str) -> None:
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    path = SNAP_DIR / f"metrics_result_{tag}_{ts}.csv"
    try:
        df.to_csv(path, encoding="utf-8-sig")
        print(f"ğŸ’¾ ìŠ¤ëƒ…ìƒ· ì €ì¥: {path}")
    except Exception as e:
        print(f"âš ï¸ ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}")

# â”€â”€ ìˆ«ì ë³€í™˜(í¼ì„¼íŠ¸/ì½¤ë§ˆ/ê³µë°±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def to_num_strict(x):
    if isinstance(x, str):
        s = x.strip().replace(",", "")
        if s.endswith("%"):
            try:
                return float(s[:-1]) / 100.0
            except Exception:
                return pd.NA
        try:
            return float(s)
        except Exception:
            return pd.NA
    return pd.to_numeric(x, errors="coerce")

# â”€â”€ í•œ ë²ˆì˜ ë°°ì¹˜ í˜¸ì¶œ(wrap) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_batch_for(codes: List[str]) -> pd.DataFrame:
    """get_metrics_for_codes ë˜í¼: ê°•í•œ íƒ€ì„ì•„ì›ƒ/í”„ë¡œì„¸ìŠ¤í’€/í”¼í´ìºì‹œ ì‚¬ìš©"""
    if not codes:
        return pd.DataFrame()

    print(f"ğŸš€ ë°°ì¹˜ ì‹œì‘: {len(codes)}ê°œ ì¢…ëª© (ì˜ˆì‹œ: {codes[:5]})")
    df = get_metrics_for_codes(
        codes,
        bgn_de="20210101",
        report_tp="annual",
        separate=True,
        latest_only=False,
        percent_format=False,
        api_key=API_KEY,

        # íŒŒì¼/ìºì‹œ
        save_each=True,
        output_dir=str(OUTPUT_DIR),

        # ì•ˆì •ì„±/ì„±ëŠ¥ ì„¤ì •(ì¤‘ìš”)
        max_workers=4,                 # ì—°ê²°/ì„œë²„ ì•ˆì •ì„± ìœ„í•´ ë™ì‹œì„± ë‚®ì¶¤
        prefer_process=True,           # í”„ë¡œì„¸ìŠ¤í’€(+ í•˜ë“œ íƒ€ì„ì•„ì›ƒ ìœ íš¨)
        per_code_timeout_sec=90,       # ì¢…ëª©ë³„ ìµœëŒ€ 90ì´ˆ
        skip_nan_heavy=True,           # NaN ê³¼ë‹¤ëŠ” ìŠ¤í‚µ
        nan_ratio_limit=0.60,
        min_non_null=5,

        # í”¼í´ ìºì‹œ: ì¬ì‹œì‘ ê°€ì†
        cache_dir=str(PKL_CACHE_DIR),
        cache_ttl=24 * 3600,
        force_refresh=False,
    )
    return df

# â”€â”€ ë¬´ê²°ì„±/ê²°ì¸¡ì¹˜ ë³´ê³ ì„œ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def produce_integrity_report(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df

    res = df.copy()

    # ê°•ì œ ìˆ«ì ë³€í™˜
    for col in ("debt_ratio", "equity_ratio"):
        if col in res.columns:
            res[col] = res[col].map(to_num_strict)

    # implied equity from debt
    if "debt_ratio" in res.columns:
        dr = res["debt_ratio"]
        res["equity_implied_from_debt"] = (1.0 / (1.0 + dr)).where(dr.notna())
    else:
        res["equity_implied_from_debt"] = pd.NA

    # filled equity ratio: ë³´ê³ ì¹˜ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ, ì—†ìœ¼ë©´ impliedë¡œ ë³´ì¶©
    if "equity_ratio" in res.columns:
        er = res["equity_ratio"].copy()
    else:
        er = pd.Series(pd.NA, index=res.index, dtype="float64")

    res["equity_ratio_filled"] = er.where(er.notna(), res["equity_implied_from_debt"])

    # Î” = (ë³´ê³ ì¹˜ ë˜ëŠ” ë³´ì¶©ì¹˜) - implied
    res["Î”_equity_ratio"] = (res["equity_ratio_filled"] - res["equity_implied_from_debt"]).astype("float64")

    # ê²°ì¸¡ì¹˜ ìš”ì•½
    na_summary = res.isna().sum().sort_values(ascending=False)
    print("\n=== ê²°ì¸¡ì¹˜ ìš”ì•½(ìƒìœ„ 15ê°œ) ===")
    print(na_summary.head(15))

    # ìƒ˜í”Œ ì¶œë ¥
    cols_show = [c for c in ["equity_ratio", "equity_ratio_filled", "debt_ratio", "equity_implied_from_debt", "Î”_equity_ratio"] if c in res.columns]
    print("\n=== ë¬´ê²°ì„± ì²´í¬ ìƒ˜í”Œ(ìƒìœ„ 10í–‰) ===")
    print(res[cols_show].head(10))

    # íŒŒì¼ë¡œ ì €ì¥
    try:
        res.to_csv(INTEGRITY_CSV, encoding="utf-8-sig", index=True)
        print(f"ğŸ§¾ ë¬´ê²°ì„± ë¦¬í¬íŠ¸ ì €ì¥: {INTEGRITY_CSV}")
    except Exception as e:
        print(f"âš ï¸ ë¬´ê²°ì„± ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    return res

# â”€â”€ ë©”ì¸ ë¡œì§: ë‚¨ì€ ì¢…ëª©ë§Œ ë°˜ë³µ ì²˜ë¦¬ + ì¤‘ê°„ ìŠ¤ëƒ…ìƒ· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    all_codes = normalize_codes(CODES)
    print(f"âœ… íƒ€ê¹ƒ ì¢…ëª© ìˆ˜: {len(all_codes)} (ì˜ˆì‹œ 5ê°œ: {all_codes[:5]})")
    print(f"ğŸ“ CSV ìºì‹œ í´ë”: {OUTPUT_DIR.resolve()}")

    # 1) ë¨¼ì € í˜„ì¬ ìºì‹œë¡œ ìŠ¤ëƒ…ìƒ·/ë¦¬ì ˆíŠ¸ ìƒì„± (ì‹¤í–‰ ì¤‘ì—ë„ ìµœì‹  ìƒíƒœ í™•ì¸ ê°€ëŠ¥)
    merged0 = merge_by_stock_cache(OUTPUT_DIR)
    if not merged0.empty:
        merged0.to_csv(RESULT_CSV, encoding="utf-8-sig")
        print(f"ğŸ“„ ì´ˆê¸° ë³‘í•©ë³¸ ì €ì¥: {RESULT_CSV} (rows={len(merged0)})")

    # 2) ë‚¨ì€ ì¢…ëª©ë§Œ ìˆ˜í–‰
    passes = 0
    max_passes = 5  # ë°©ì–´ì  ìƒí•œ
    while passes < max_passes:
        if _stop_requested:
            print("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ìœ¼ë¡œ ë£¨í”„ ì¢…ë£Œ")
            break

        todo = compute_todo_codes(all_codes, OUTPUT_DIR)
        if not todo:
            print("ğŸ‰ ì²˜ë¦¬í•  ë‚¨ì€ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            break

        passes += 1
        print(f"\nğŸ” íŒ¨ìŠ¤ {passes}/{max_passes} â€” ë‚¨ì€ ì¢…ëª©: {len(todo)}")

        try:
            _ = run_batch_for(todo)
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            time.sleep(5)
            continue

        # íŒ¨ìŠ¤ ì§í›„ ë³‘í•©/ìŠ¤ëƒ…ìƒ·
        merged = merge_by_stock_cache(OUTPUT_DIR)
        if not merged.empty:
            merged.to_csv(RESULT_CSV, encoding="utf-8-sig")
            print(f"âœ… ë³‘í•©/ì €ì¥ ì™„ë£Œ: {RESULT_CSV} (rows={len(merged)})")
            save_snapshot(merged, tag=f"pass{passes}")
        else:
            print("âš ï¸ ë³‘í•© ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìºì‹œ/ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        # ì‹¤íŒ¨/ìŠ¤í‚µ í˜„í™© ì•ˆë‚´
        if FAILED_CSV.exists():
            try:
                failed = pd.read_csv(FAILED_CSV, dtype=str)
                if not failed.empty:
                    ex = failed.head(10).to_dict(orient="list")
                    print(f"âš ï¸ ì‹¤íŒ¨ ì¢…ëª© {len(failed)}ê°œ (ì˜ˆì‹œ 10ê°œ): {ex}")
            except Exception:
                pass
        if SKIPPED_CSV.exists():
            try:
                skipped = pd.read_csv(SKIPPED_CSV, dtype=str)
                if not skipped.empty:
                    ex = skipped.head(10).to_dict(orient="list")
                    print(f"â„¹ï¸ ìŠ¤í‚µ ì¢…ëª© {len(skipped)}ê°œ (ì˜ˆì‹œ 10ê°œ): {ex}")
            except Exception:
                pass

    # 3) ì¢…ë£Œ ì§ì „ ìµœì¢… ë³‘í•©/ì €ì¥ (ì•ˆì „)
    final_df = merge_by_stock_cache(OUTPUT_DIR)
    if not final_df.empty:
        final_df.to_csv(RESULT_CSV, encoding="utf-8-sig")
        print(f"\nğŸ ìµœì¢… ì €ì¥ ì™„ë£Œ: {RESULT_CSV} (rows={len(final_df)})")
    else:
        print("\nâ— ìµœì¢… ë³‘í•©ë³¸ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì‹¤í–‰ ë¡œê·¸/ìºì‹œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # 4) ë¬´ê²°ì„±/ê²°ì¸¡ì¹˜ ë¦¬í¬íŠ¸
    if not final_df.empty:
        try:
            _ = produce_integrity_report(final_df)
        except Exception as e:
            print(f"âš ï¸ ë¬´ê²°ì„± ì²´í¬ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì‚¬ìš©ì ì¤‘ë‹¨. ë§ˆì§€ë§‰ ë³‘í•©ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        try:
            final_df = merge_by_stock_cache(OUTPUT_DIR)
            if not final_df.empty:
                final_df.to_csv(RESULT_CSV, encoding="utf-8-sig")
                print(f"ğŸ’¾ ì¤‘ë‹¨ ì „ ì €ì¥: {RESULT_CSV} (rows={len(final_df)})")
        except Exception as e:
            print(f"âš ï¸ ì¤‘ë‹¨ ì „ ì €ì¥ ì‹¤íŒ¨: {e}")
        sys.exit(1)
