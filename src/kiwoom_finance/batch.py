# src/kiwoom_finance/batch.py
from __future__ import annotations

import time
import traceback
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, TimeoutError
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional
import hashlib
import json
import re
import sys

import pandas as pd
from tqdm import tqdm

# ë‚´ë¶€ ëª¨ë“ˆ
from .dart_client import extract_fs, find_corp, init_dart
from .preprocess import preprocess_all
from .metrics import compute_metrics_df_flat_kor

# dart_fss íŠ¹ì • ì˜ˆì™¸(ì—°ê²°ì¬ë¬´ ë¯¸ë°œê²¬) ì‹ë³„ìš©
try:
    from dart_fss.errors.errors import NotFoundConsolidated  # type: ignore
except Exception:  # dart_fss ë¯¸ë¡œë”© í™˜ê²½ ëŒ€ë¹„
    NotFoundConsolidated = tuple()  # noqa: N816


# -----------------------
# ê¸°ë³¸ ì‚°ì¶œ ì»¬ëŸ¼(ì¡´ì¬ ì‹œì—ë§Œ ì„ íƒ)
# -----------------------
DEFAULT_COLS = [
    "debt_ratio", "equity_ratio", "debt_dependency_ratio",
    "current_ratio", "quick_ratio", "interest_coverage_ratio",
    "ebitda_to_total_debt", "cfo_to_total_debt", "free_cash_flow",
    "operating_margin", "roa", "roe", "net_profit_margin",
    "total_asset_turnover", "accounts_receivable_turnover", "inventory_turnover",
    "sales_growth_rate", "operating_income_growth_rate", "total_asset_growth_rate",
]


@dataclass
class WorkerConfig:
    bgn_de: str
    report_tp: str
    separate: bool
    latest_only: bool
    percent_format: bool
    api_key: Optional[str] = None
    retries: int = 3
    throttle_sec: float = 1.2  # DART ë¶€í•˜ ë°©ì§€ìš© sleep (1.0 -> 1.2)


def _select_existing_cols(df: pd.DataFrame) -> pd.DataFrame:
    """DEFAULT_COLS ì¤‘ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ì¶”ë ¤ì„œ ë°˜í™˜"""
    keep = [c for c in DEFAULT_COLS if c in df.columns]
    return df[keep].copy() if keep else pd.DataFrame(index=df.index)


def _try_extract_with_fallback(corp, cfg: WorkerConfig):
    """
    1ì°¨: cfg.separate ì„¤ì •ëŒ€ë¡œ ì‹œë„
    2ì°¨: NotFoundConsolidated ì‹œ ë°˜ëŒ€ separateë¡œ í´ë°± ì‹œë„
    """
    try:
        return extract_fs(
            corp,
            bgn_de=cfg.bgn_de,
            report_tp=cfg.report_tp,
            separate=cfg.separate,
        )
    except Exception as e:
        # ì—°ê²°/ë³„ë„ ë¯¸ë°œê²¬ ì‹œ í´ë°±
        is_nfc = False
        if NotFoundConsolidated and isinstance(e, NotFoundConsolidated):  # ì •ì‹ íŒë³„
            is_nfc = True
        elif "NotFoundConsolidated" in f"{type(e)} {e}":  # ë¬¸ìì—´ íŒë³„(ë³´ì¡°)
            is_nfc = True

        if is_nfc:
            alt = not cfg.separate
            tqdm.write(
                f"ğŸ” [{getattr(corp, 'stock_code', '???')}] "
                f"{'ì—°ê²°' if cfg.separate else 'ë³„ë„'} ì¬ë¬´ ë¯¸ë°œê²¬ â†’ "
                f"{'ë³„ë„' if alt else 'ì—°ê²°'} ì¬ë¬´ë¡œ ì¬ì‹œë„"
            )
            return extract_fs(
                corp,
                bgn_de=cfg.bgn_de,
                report_tp=cfg.report_tp,
                separate=alt,
            )
        raise  # ê¸°íƒ€ ì˜ˆì™¸ëŠ” ìƒìœ„ ì¬ì‹œë„ ëŒ€ìƒìœ¼ë¡œ


# =============
# í’ˆì§ˆ/ìºì‹œ í—¬í¼
# =============
def _resolve_output_dir_safely(output_dir: str) -> Path:
    """
    - ìƒëŒ€ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ê°•ì œ
    - ì¡°ìƒ ê²½ë¡œ ì¤‘ 'íŒŒì¼'ì´ ë¼ì–´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ *_output ìœ¼ë¡œ ìš°íšŒ
    - ìµœì¢… ê²½ë¡œê°€ íŒŒì¼ì´ë©´ *_dir ë¡œ ìš°íšŒ
    - ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ .kiwoom_out/<ì›ë˜ë§ˆì§€ë§‰í´ë”> ë¡œ í´ë°±
    """
    def _abs(p: Path) -> Path:
        return p if p.is_absolute() else (Path.cwd() / p)

    try:
        p = _abs(Path(output_dir))

        # 1) ì¡°ìƒ ê²½ë¡œ ê²€ì‚¬ (ë£¨íŠ¸â†’ë¶€ëª¨ ìˆœì„œ)
        ancestors = list(p.parents)[::-1]  # ë£¨íŠ¸ì— ê°€ê¹Œìš´ ê²ƒë¶€í„°
        for anc in ancestors:
            if anc.exists() and not anc.is_dir():
                alt_anc = Path(str(anc) + "_output")
                rel = p.relative_to(anc)
                p = alt_anc / rel
                tqdm.write(f"âš ï¸ ì¡°ìƒ ê²½ë¡œê°€ íŒŒì¼ì…ë‹ˆë‹¤: '{anc}' â†’ '{alt_anc}'ë¡œ ìš°íšŒí•©ë‹ˆë‹¤.")
                break

        # 2) ìµœì¢… ê²½ë¡œê°€ íŒŒì¼ì´ë©´ *_dirë¡œ ìš°íšŒ
        if p.exists() and not p.is_dir():
            alt_p = Path(str(p) + "_dir")
            tqdm.write(f"âš ï¸ ì¶œë ¥ ê²½ë¡œê°€ íŒŒì¼ì…ë‹ˆë‹¤: '{p}' â†’ '{alt_p}'ë¡œ ìš°íšŒí•©ë‹ˆë‹¤.")
            p = alt_p

        # 3) ë””ë ‰í„°ë¦¬ ìƒì„±
        p.mkdir(parents=True, exist_ok=True)
        tqdm.write(f"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {p}")
        return p

    except Exception as e:
        # 4) ìµœì¢… í´ë°±
        try:
            last = Path(output_dir).name or "by_stock"
            fallback = Path.cwd() / ".kiwoom_out" / last
            fallback.mkdir(parents=True, exist_ok=True)
            tqdm.write(f"âš ï¸ ê²½ë¡œ ìƒì„± ì‹¤íŒ¨({type(e).__name__}): '{output_dir}' â†’ í´ë°± '{fallback}' ì‚¬ìš©")
            return fallback
        except Exception as e2:
            cwd = Path.cwd()
            tqdm.write(f"â— í´ë°±ë„ ì‹¤íŒ¨({type(e2).__name__}). í˜„ì¬ ê²½ë¡œ ì‚¬ìš©: '{cwd}'")
            return cwd


def _resolve_cache_dir(cache_dir: str | Path | None) -> Path | None:
    if cache_dir is None:
        return None
    cache_root = Path(cache_dir).expanduser()
    cache_root.mkdir(parents=True, exist_ok=True)
    return cache_root

def _build_cache_key(
    code: str,
    bgn_de: str,
    report_tp: str,
    separate: bool,
    latest_only: bool,
    percent_format: bool,
) -> str:
    safe_code = re.sub(r"[^0-9A-Za-z]+", "_", code).strip("_") or "code"
    payload = {
        "code": code,
        "bgn_de": bgn_de,
        "report_tp": report_tp,
        "separate": separate,
        "latest_only": latest_only,
        "percent_format": percent_format,
    }
    raw = json.dumps(payload, sort_keys=True, separators=(",", ":"))
    digest = hashlib.sha1(raw.encode("utf-8")).hexdigest()
    return f"{safe_code}_{digest}"

def build_cache_key_public(**kwargs) -> str:
    """ì™¸ë¶€ ë„êµ¬/ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ìºì‹œ í‚¤ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”."""
    return _build_cache_key(**kwargs)

def _cache_file_path(cache_root: Path, cache_key: str) -> Path:
    return cache_root / f"{cache_key}.pkl"

def _load_cached_frame(
    cache_root: Path,
    cache_key: str,
    ttl_seconds: float | None,
) -> pd.DataFrame | None:
    cache_path = _cache_file_path(cache_root, cache_key)
    if not cache_path.exists():
        return None
    if ttl_seconds is not None:
        age = time.time() - cache_path.stat().st_mtime
        if age > ttl_seconds:
            return None
    try:
        cached = pd.read_pickle(cache_path)
    except Exception:
        return None
    if not isinstance(cached, pd.DataFrame):
        return None
    return cached

def _store_cached_frame(cache_root: Path, cache_key: str, df: pd.DataFrame) -> None:
    cache_path = _cache_file_path(cache_root, cache_key)
    tmp_path = cache_path.parent / f"{cache_path.name}.tmp"
    df.to_pickle(tmp_path)
    tmp_path.replace(cache_path)

def _load_cached_csv(code: str, output_dir_path: Path) -> pd.DataFrame | None:
    """by_stock/<code>.csv ìµœì‹ ë³¸ ì½ê¸° ì‹œë„ + ìˆ«ì ìºìŠ¤íŒ…"""
    csv_path = output_dir_path / f"{code}.csv"
    if not csv_path.exists():
        return None
    try:
        # ì¸ë±ìŠ¤/ë¬¸ì ë³´ì¡´
        df = pd.read_csv(csv_path, index_col=0, dtype=str)
        if df is None or df.empty:
            return None
        # ìˆ«ì ì¹¼ëŸ¼ë§Œ ë‹¤ì‹œ floatë¡œ ìºìŠ¤íŒ…
        num_cols = [c for c in df.columns if c != "stock_code"]
        df[num_cols] = df[num_cols].apply(pd.to_numeric, errors="coerce")
        return df
    except Exception:
        return None


def _quality_ok(df: pd.DataFrame, nan_ratio_limit: float, min_non_null: int) -> bool:
    """
    ìˆ«ìí˜•ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥í•œ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ 'ì˜ ì±„ì›Œì§„' í–‰ì„ ë³´ë©°
    NaN ë¹„ìœ¨ì´ ì„ê³„ê°’ ì´í•˜ì´ê³ , ìµœì†Œ ì±„ì›Œì§„ ì—´ ìˆ˜ë¥¼ ë§Œì¡±í•˜ë©´ í†µê³¼.
    """
    if df is None or df.empty:
        return False

    # ìˆ«ìí˜• í›„ë³´ ì»¬ëŸ¼ ì„ ì •
    cand: list[str] = []
    for c in df.columns:
        if c == "stock_code":
            continue
        ser = pd.to_numeric(df[c], errors="coerce")
        if ser.notna().any():
            cand.append(c)

    if not cand:
        return False

    sub = df[cand].apply(pd.to_numeric, errors="coerce")
    non_null_max = int(sub.notna().sum(axis=1).max())
    nan_ratio = 1.0 - (non_null_max / len(cand))
    return (nan_ratio <= nan_ratio_limit) and (non_null_max >= min_non_null)


# ============
# ì›Œì»¤ (ë‹¨ì¼ ì¢…ëª©)
# ============
def _run_worker(code: str, cfg: WorkerConfig) -> pd.DataFrame | None:
    """
    ê°œë³„ ì¢…ëª© ì²˜ë¦¬:
    - (ë©€í‹°í”„ë¡œì„¸ìŠ¤ í˜¸í™˜) ì›Œì»¤ ë‚´ë¶€ì—ì„œ DART ì´ˆê¸°í™”
    - corp ì¡°íšŒ â†’ filings ì¶”ì¶œ(ì—°ê²°â†’ë³„ë„ í´ë°±) â†’ ì „ì²˜ë¦¬ â†’ ì§€í‘œ ì‚°ì¶œ â†’ ì»¬ëŸ¼ í•„í„°
    - latest_onlyë©´ ìµœì‹  1ê°œë§Œ
    ì‹¤íŒ¨ ì‹œ None
    """
    # í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œ ì–´ë””ì„œë“  ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”
    init_dart(cfg.api_key)

    for attempt in range(1, cfg.retries + 1):
        try:
            # ë¶€í•˜ ë°©ì§€
            time.sleep(cfg.throttle_sec)

            corp = find_corp(code)
            if corp is None:
                raise ValueError(f"corp not found for {code}")

            # FS ì¶”ì¶œ(ì—°ê²°â†’ë³„ë„ í´ë°± ë‚´ì¥)
            fs = _try_extract_with_fallback(corp, cfg)

            # í‘œì¤€í™”ëœ 4í‘œ ìƒì„±
            bs, is_, cis, cf = preprocess_all(fs)

            # ì§€í‘œ ê³„ì‚°
            df = compute_metrics_df_flat_kor(bs, is_, cis, cf, key_cols=None)

            # ìš”ì²­ëœ ì»¬ëŸ¼ë§Œ ì¶”ë¦¼(ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ)
            df = _select_existing_cols(df)

            if df.empty:
                tqdm.write(f"âš ï¸ [{code}] ë°ì´í„° ì—†ìŒ (ë¹ˆ DataFrame)")
                return None

            # ìµœì‹  1ê°œë§Œ
            if cfg.latest_only:
                df = df.sort_index(ascending=False).iloc[[0]]
                df.index = [code]
            else:
                df.index = [f"{code}_{i}" for i in range(len(df))]

            df.index.name = "stock_code"
            return df

        except Exception as e:
            tqdm.write(f"âš ï¸ [{code}] ì‹œë„ {attempt}/{cfg.retries} ì‹¤íŒ¨: {type(e).__name__}: {e}")
            traceback.print_exc(limit=1)  # ìŠ¤íƒ ì¶”ì  ê³¼ë‹¤ ì¶œë ¥ ë°©ì§€
            time.sleep(0.9 * attempt)     # ì§€ìˆ˜ ë°±ì˜¤í”„(ì†Œí­ ìƒí–¥)

    tqdm.write(f"âŒ [{code}] {cfg.retries}íšŒ ì‹¤íŒ¨ í›„ ê±´ë„ˆëœ€.")
    return None


# =============
# ë©”ì¸ ì—”íŠ¸ë¦¬
# =============
def get_metrics_for_codes(
    codes: List[str],
    bgn_de: str = "20210101",
    report_tp: str = "annual",
    separate: bool = True,
    latest_only: bool = False,
    percent_format: bool = False,  # í˜„ì¬ ìˆ«ìí˜• ìœ ì§€ ê¸°ë³¸
    api_key: Optional[str] = None,
    max_workers: int = 6,
    save_each: bool = False,
    output_dir: str = "artifacts/by_stock",
    # â”€â”€ ìƒˆ ì˜µì…˜: í’ˆì§ˆ/íƒ€ì„ì•„ì›ƒ/ì‹¤í–‰ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    per_code_timeout_sec: int | None = 150,   # ì¢…ëª©ë‹¹ í•˜ë“œ íƒ€ì„ì•„ì›ƒ(ì´ˆ). None/0ì´ë©´ ë¯¸ì ìš©
    skip_nan_heavy: bool = True,              # NaN ê³¼ë‹¤ ê²°ê³¼ ìŠ¤í‚µ
    nan_ratio_limit: float = 0.60,            # NaN â‰¤ 60%
    min_non_null: int = 5,                    # ìµœì†Œ ì±„ì›Œì§„ ì§€í‘œ 5ê°œ
    prefer_process: bool = False,             # ğŸ”§ Windows ê¸°ë³¸: ThreadPool (ProcessPoolì€ __main__ ê°€ë“œ í•„ìš”)
    # â”€â”€ í”¼í´ ìºì‹œ ì˜µì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cache_dir: str | Path | None = None,
    cache_ttl: float | int | None = None,
    force_refresh: bool = False,
) -> pd.DataFrame:
    """
    ë³µìˆ˜ ì¢…ëª© ì¬ë¬´ì§€í‘œë¥¼ ë³‘ë ¬ ìˆ˜ì§‘
    - CSV ì €ì¥ ìºì‹œ(save_each+output_dir) + í”¼í´ ìºì‹œ(cache_dir) í˜¼ìš© ì§€ì›
    - í’ˆì§ˆ í•„í„°ë¡œ NaN ê³¼ë‹¤ ì¢…ëª© ìŠ¤í‚µ ê°€ëŠ¥
    - ì¢…ëª©ë³„ í•˜ë“œ íƒ€ì„ì•„ì›ƒ
    - Windows ê¸°ë³¸ ThreadPool ì‚¬ìš©(ë©€í‹°í”„ë¡œì„¸ìŠ¤ëŠ” __main__ ë³´í˜¸ í•„ìš”)
    """
    if not codes:
        return pd.DataFrame(columns=DEFAULT_COLS)

    # ğŸ”§ ì•ˆì „í•œ ë””ë ‰í„°ë¦¬ ìƒì„± (artifactsê°€ íŒŒì¼ì´ì–´ë„ ìš°íšŒ)
    output_dir_path = _resolve_output_dir_safely(output_dir)

    frames: list[pd.DataFrame] = []
    failed: list[str] = []
    skipped: list[tuple[str, str]] = []  # (code, reason)

    cfg = WorkerConfig(
        bgn_de=bgn_de,
        report_tp=report_tp,
        separate=separate,
        latest_only=latest_only,
        percent_format=percent_format,
        api_key=api_key,
        retries=3,
        throttle_sec=1.2,
    )

    # ===== í”¼í´ ìºì‹œ ìš°ì„  ì¡°íšŒ =====
    cache_root = _resolve_cache_dir(cache_dir)
    ttl_seconds = float(cache_ttl) if cache_ttl is not None else None

    # ì œì¶œ ëŒ€ìƒ ê²°ì •: CSV ìºì‹œ â†’ í”¼í´ ìºì‹œ â†’ API
    submit_codes: list[str] = []

    # 1) CSV ìºì‹œ ì²´í¬ (í•˜ìœ„ í˜¸í™˜)
    if save_each:
        for code in codes:
            csv_df = _load_cached_csv(code, output_dir_path)
            if csv_df is not None:
                if (not skip_nan_heavy) or _quality_ok(csv_df, nan_ratio_limit, min_non_null):
                    frames.append(csv_df)
                    tqdm.write(f"âœ… [CSV ìºì‹œ ì‚¬ìš©] {code}")
                    continue
                else:
                    tqdm.write(f"âš ï¸ [CSV ìºì‹œ í’ˆì§ˆë¶ˆëŸ‰] {code} â†’ ì¬ì‹œë„ ì˜ˆì •")
            submit_codes.append(code)
    else:
        submit_codes = list(codes)

    # 2) í”¼í´ ìºì‹œ ì²´í¬
    frames_ordered: list[pd.DataFrame | None] = [None] * len(submit_codes)
    cache_keys: dict[int, str] = {}
    codes_to_fetch: list[tuple[int, str]] = []

    if cache_root is not None:
        for idx, code in enumerate(submit_codes):
            ck = _build_cache_key(
                code=code,
                bgn_de=bgn_de,
                report_tp=report_tp,
                separate=separate,
                latest_only=latest_only,
                percent_format=percent_format,
            )
            cache_keys[idx] = ck
            if not force_refresh:
                cached_df = _load_cached_frame(cache_root, ck, ttl_seconds)
                if cached_df is not None:
                    if (not skip_nan_heavy) or _quality_ok(cached_df, nan_ratio_limit, min_non_null):
                        frames_ordered[idx] = cached_df
                        tqdm.write(f"âœ… [í”¼í´ ìºì‹œ íˆíŠ¸] {code}")
                        continue
                    else:
                        tqdm.write(f"âš ï¸ [í”¼í´ ìºì‹œ í’ˆì§ˆë¶ˆëŸ‰] {code} â†’ ì¬ì‹œë„ ì˜ˆì •")
            codes_to_fetch.append((idx, code))
    else:
        codes_to_fetch = list(enumerate(submit_codes))

    # 3) API ë³‘ë ¬ ì²˜ë¦¬
    def _submit_and_collect(to_fetch: list[tuple[int, str]]):
        if not to_fetch:
            return

        Executor = ProcessPoolExecutor if (prefer_process and sys.platform != "win32") else ThreadPoolExecutor

        with Executor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(_run_worker, code, cfg): (idx, code)
                for idx, code in to_fetch
            }
            pbar = tqdm(total=len(futures), desc="ğŸ“Š ì¢…ëª© ì²˜ë¦¬ ì¤‘", ncols=100, dynamic_ncols=False)
            for future in as_completed(futures):
                idx, code = futures[future]
                try:
                    if per_code_timeout_sec and per_code_timeout_sec > 0:
                        df = future.result(timeout=per_code_timeout_sec)
                    else:
                        df = future.result()

                    if df is not None and not df.empty:
                        # í’ˆì§ˆ í•„í„°
                        if skip_nan_heavy and not _quality_ok(df, nan_ratio_limit, min_non_null):
                            skipped.append((code, "nan_heavy"))
                            tqdm.write(f"â­ï¸  [{code}] NaN ê³¼ë‹¤ë¡œ ìŠ¤í‚µ")
                        else:
                            frames_ordered[idx] = df
                            # í”¼í´ ìºì‹œ ì €ì¥
                            if cache_root is not None:
                                _store_cached_frame(cache_root, cache_keys[idx], df)
                            # CSV ìºì‹œ ì €ì¥(ì˜µì…˜)
                            if save_each:
                                (output_dir_path / f"{code}.csv").parent.mkdir(parents=True, exist_ok=True)
                                df.to_csv(output_dir_path / f"{code}.csv", encoding="utf-8-sig")
                    else:
                        failed.append(code)
                except TimeoutError:
                    failed.append(code)
                    tqdm.write(f"â±ï¸  [{code}] íƒ€ì„ì•„ì›ƒ({per_code_timeout_sec}s)ìœ¼ë¡œ ì‹¤íŒ¨ ì²˜ë¦¬")
                except Exception as e:
                    tqdm.write(f"âŒ [{code}] ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e}")
                    failed.append(code)
                finally:
                    pbar.update(1)
            pbar.close()

    _submit_and_collect(codes_to_fetch)

    # ì‹¤íŒ¨/ìŠ¤í‚µ ëª©ë¡ ì €ì¥
    report_dir = _resolve_output_dir_safely("artifacts")
    if failed:
        fail_path = report_dir / "failed_codes.csv"
        pd.DataFrame({"failed_code": failed}).to_csv(fail_path, index=False, encoding="utf-8-sig")
        print(f"\nâš ï¸ ì‹¤íŒ¨í•œ ì¢…ëª© {len(failed)}ê°œ â†’ {fail_path} ì €ì¥ë¨")
    if skipped:
        skip_path = report_dir / "skipped_codes.csv"
        pd.DataFrame(skipped, columns=["code", "reason"]).to_csv(skip_path, index=False, encoding="utf-8-sig")
        print(f"â„¹ï¸ ìŠ¤í‚µëœ ì¢…ëª© {len(skipped)}ê°œ â†’ {skip_path} ì €ì¥ë¨")

    # ê²°ê³¼ ë³‘í•©
    from_cache = [f for f in frames_ordered if f is not None]
    frames.extend(from_cache)

    if not frames:
        print("â— ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=DEFAULT_COLS)

    result = pd.concat(frames)
    print(f"\nâœ… ì™„ë£Œ! ì´ {len(result)}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ.")
    return result
