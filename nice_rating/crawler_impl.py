# -*- coding: utf-8 -*-
"""
NICE ì‹ ìš©í‰ê°€ - ë³‘ë ¬ ê²€ìƒ‰ í¬ë¡¤ëŸ¬(ìºì‹œ + ë¦¬íŠ¸ë¼ì´ ê°•í™”)
- ë””ìŠ¤í¬ ìºì‹œ: data/output/*.csv ë¥¼ ëª¨ì•„ ì¸ë±ìŠ¤ êµ¬ì¶• â†’ ìºì‹œ íˆíŠ¸ ì‹œ ì¦‰ì‹œ ë°˜í™˜(í¬ë¡¤ë§ ìƒëµ)
- ëŸ°íƒ€ì„ ìºì‹œ: í•œ ë²ˆ ìˆ˜ì§‘í•œ ê²°ê³¼ëŠ” ê°™ì€ ì‹¤í–‰ ì¤‘ ì¬ì‚¬ìš©
- 1ë¼ìš´ë“œ: ë³‘ë ¬(ìŠ¤ë ˆë“œ) í¬ë¡¤ë§
- 2ë¼ìš´ë“œ(ìµœì¢… ë¦¬íŠ¸ë¼ì´): 1ë¼ìš´ë“œ 'ê²€ìƒ‰ ì‹¤íŒ¨/ê²°ê³¼ ì—†ìŒ'ë§Œ ì§ë ¬ ì¬ê²€ìƒ‰(ëŒ€ê¸°ì‹œê°„ ìƒí–¥, í‘œì—ì„œ ì±„ê¶Œê°’ ì—†ì–´ë„ cmpCdë§Œ ìˆìœ¼ë©´ ìƒì„¸ ì¬íŒŒì‹±)
- ìƒì„¸ í˜ì´ì§€ ìš”ì²­ì€ 500/502/503/504/429 ë“± ì¼ì‹œ ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„
- ì›Œì»¤ ì„¸ì…˜ ê¹¨ì§€ë©´ ë“œë¼ì´ë²„ ì¬ìƒì„± í›„ 1íšŒ ì¬ì‹œë„
"""
# í•„ìš” íŒ¨í‚¤ì§€
# pip install selenium webdriver-manager beautifulsoup4 requests pandas

import os
import re
import time
import math
import unicodedata
import threading
import argparse
from datetime import datetime
from difflib import SequenceMatcher
from urllib.parse import urlparse, parse_qs, quote_plus
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
import pandas as pd
from bs4 import BeautifulSoup

# Selenium
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

# ë“±ê¸‰ ë¬¸ìì—´ ì •ë¦¬
def _clean_grade(g: str | None) -> str:
    if not g:
        return ""
    g = str(g).strip()
    # ë“±ê¸‰ ë’¤ì— ë¶™ì€ ë¶ˆí•„ìš”í•œ ì‰¼í‘œ/ê³µë°± ì œê±° (ì˜ˆ: "A," -> "A")
    g = re.sub(r"[\s,;]+$", "", g)
    return g

def build_two_column_df(companies: list[str], rows_all: list[dict]) -> pd.DataFrame:
    """
    companies.txt ìˆœì„œë¥¼ ë³´ì¡´í•˜ì—¬
    [íšŒì‚¬ëª…(ìš”ì²­ê²€ìƒ‰ì–´), ë“±ê¸‰] 2ì—´ DataFrameì„ ë§Œë“ ë‹¤.
    - ë™ì¼ ê²€ìƒ‰ì–´ì—ì„œ ë‹¤ìˆ˜ ê¸°ì—…ì´ ë°œê²¬ë˜ë©´ ë“±ê¸‰ìˆìŒ/ì´ë¦„ì¼ì¹˜/í¬í•¨/ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ 1ê±´ ì„ íƒ
    """
    # ìˆ˜ì§‘ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë“±ê¸‰ìœ¼ë¡œ ë°˜í™˜
    if not rows_all:
        return pd.DataFrame({"íšŒì‚¬ëª…": companies, "ë“±ê¸‰": [""] * len(companies)})

    df_rows = pd.DataFrame(rows_all)
    # í•„ìš”í•œ ì»¬ëŸ¼ ë³´ì •
    for c in ["ìš”ì²­ê²€ìƒ‰ì–´", "íšŒì‚¬ëª…", "ë“±ê¸‰"]:
        if c not in df_rows.columns:
            df_rows[c] = ""
    # ë“±ê¸‰ ì •ë¦¬
    df_rows["ë“±ê¸‰"] = df_rows["ë“±ê¸‰"].map(_clean_grade)

    results = []
    for q in companies:
        cand = df_rows[df_rows["ìš”ì²­ê²€ìƒ‰ì–´"] == q]
        if cand.empty:
            results.append({"íšŒì‚¬ëª…": q, "ë“±ê¸‰": ""})
            continue

        q_norm = normalize_text(aliasize(q))

        # ìŠ¤ì½”ì–´ ê³„ì‚°: ë“±ê¸‰ ìœ ë¬´>ì •í™•ì¼ì¹˜>ë¶€ë¶„í¬í•¨>ìœ ì‚¬ë„
        def _score(row) -> float:
            comp = str(row.get("íšŒì‚¬ëª…", ""))
            grade = _clean_grade(row.get("ë“±ê¸‰", ""))
            comp_norm = normalize_text(aliasize(comp))
            sim = name_similarity(comp_norm, q_norm)
            s = 0.0
            if grade: s += 2.0                # ë“±ê¸‰ ìˆëŠ” í•­ëª© ê°€ì¤‘ì¹˜
            if comp_norm == q_norm: s += 1.5  # ì •í™• ì¼ì¹˜
            if (q_norm and comp_norm) and (q_norm in comp_norm or comp_norm in q_norm):
                s += 1.0                      # ë¶€ë¶„ í¬í•¨
            s += 0.5 * sim                    # ìœ ì‚¬ë„ ë³´ì •
            return s

        # ìµœìƒ ìŠ¤ì½”ì–´ 1ê±´ ì„ íƒ
        best = cand.copy()
        best["__score"] = best.apply(_score, axis=1)
        best = best.sort_values("__score", ascending=False).iloc[0]
        results.append({"íšŒì‚¬ëª…": q, "ë“±ê¸‰": _clean_grade(best.get("ë“±ê¸‰", ""))})

    return pd.DataFrame(results)


# -------------------- ê¸°ë³¸ ì„¤ì • --------------------
BASE = "https://www.nicerating.com"
HOME = f"{BASE}/"

SEARCH_TIMEOUT = 20            # 1ë¼ìš´ë“œ ê¸°ë³¸ wait(ì´ˆ)
REQUEST_TIMEOUT = 15
HEADLESS = True                # ë””ë²„ê¹… ì‹œ False
MAX_WORKERS = 4                # 3~6 ê¶Œì¥
BATCH_SIZE_AUTO = True         # Trueë©´ íšŒì‚¬ ìˆ˜/ì›Œì»¤ ìˆ˜ë¡œ ìë™ ë¶„í• 

# ìµœì¢… ë¦¬íŠ¸ë¼ì´(ì§ë ¬) ì„¤ì •
FINAL_RETRY = True
FINAL_RETRY_TIMEOUT = 35       # ë¦¬íŠ¸ë¼ì´ wait(ì´ˆ) - ë” ê¸¸ê²Œ
FINAL_RETRY_SLEEP = 1.2        # ëª©ë¡í‘œ ì¬íŒŒì‹± ëŒ€ê¸°(ì´ˆ)
REQUIRE_BOND_IN_TABLE_FIRST = True   # 1ë¼ìš´ë“œ: í‘œì—ì„œ 'ì±„ê¶Œ' ì¹¼ëŸ¼ ìˆëŠ” í–‰ë§Œ
REQUIRE_BOND_IN_TABLE_RETRY = False  # 2ë¼ìš´ë“œ: í‘œì—ì„œ ì±„ê¶Œ ì¹¼ëŸ¼ ì—†ì–´ë„ cmpCdë§Œ ìˆìœ¼ë©´ ìƒì„¸ ì¬íŒŒì‹±

# ìºì‹œ ì„¤ì •
USE_CACHE_FIRST = True         # âœ… ìºì‹œ ìš°ì„  ì‚¬ìš©
CACHE_MATCH_EXACT_ONLY = True  # True: ì •ê·œí™”í•œ ì´ë¦„/ìš”ì²­ê²€ìƒ‰ì–´ 'ì •í™•' ì¼ì¹˜ë§Œ ë§¤ì¹­(ê¶Œì¥)
# Falseë¡œ ë°”ê¾¸ë©´ í¼ì§€ ë§¤ì¹­(ìœ ì‚¬ë„)ë„ ì‹œë„í•˜ë„ë¡ í™•ì¥í•  ìˆ˜ ìˆìŒ(ì•„ë˜ í•¨ìˆ˜ ë‚´ ì£¼ì„ ì°¸ê³ )

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/118.0.0.0 Safari/537.36"
    ),
}
TRANSIENT_STATUS = {500, 502, 503, 504, 429}

# -------------------- ë¬¸ìì—´ ë³´ì •/ìœ ì‚¬ë„ --------------------
ALIASES = [
    (re.compile(r"\bì—ìŠ¤ì¼€ì´", re.I), "SK"),
    (re.compile(r"\bì—˜ì§€", re.I), "LG"),
    (re.compile(r"\bì”¨ì œì´", re.I), "CJ"),
    (re.compile(r"\bì¼€ì´í‹°\b", re.I), "KT"),
    (re.compile(r"\bì¼€ì´ë¹„\b", re.I), "KB"),
    (re.compile(r"\bì—”ì—ì´ì¹˜\b", re.I), "NH"),
    (re.compile(r"\bì—ì´ì¹˜ë””", re.I), "HD"),
    (re.compile(r"\bì—ì´ì¼€ì´\b", re.I), "AK"),
    (re.compile(r"\bí¬ìŠ¤ì½”", re.I), "POSCO"),
    (re.compile(r"\bì”¨ì œì´", re.I), "CJ"),
    (re.compile(r"\bì¼€ì´ë¹„", re.I), "KB"),
    (re.compile(r"\bì§€ì—ìŠ¤", re.I), "GS"),
    (re.compile(r"\bì•„ì´ë¹„", re.I), "IB"),
    (re.compile(r"\bë¹„ì—‘ìŠ¤", re.I), "BX"),
    (re.compile(r"\bì—ì´ì•„ì´", re.I), "AI"),
]
def aliasize(s: str) -> str:
    for pat, rep in ALIASES:
        s = pat.sub(rep, s)
    return re.sub(r"\s*\(ì£¼\)|ãˆœ", "", s).strip()

def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = unicodedata.normalize("NFKC", s).strip().lower()
    s = re.sub(r"(ì£¼ì‹íšŒì‚¬|ãˆœ|\(ì£¼\)|ìœ í•œíšŒì‚¬|í™€ë”©ìŠ¤)", " ", s)
    s = re.sub(r"[\(\)\[\]\{\}]", " ", s)
    return re.sub(r"\s+", " ", s)

def name_similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, normalize_text(a), normalize_text(b)).ratio()


# -------------------- ìƒì„¸ í˜ì´ì§€ íŒŒì„œ(ê¸°ì¡´ ë¡œì§ ìœ ì§€) --------------------
def _extract_name_from_tbl_type99(soup: BeautifulSoup) -> str | None:
    tbl = soup.select_one("div.tbl_type99 table")
    if not tbl:
        return None
    tbody = tbl.find("tbody")
    first_td = (tbody.find("td") if tbody else tbl.find("td"))
    return first_td.get_text(" ", strip=True) if first_td else None

def _extract_grade_primary_tbl1(soup: BeautifulSoup) -> str | None:
    table = soup.find("table", {"id": "tbl1"})
    if not table:
        return None
    tds = table.find_all("td", class_="cell_txt01")
    if not tds:
        return None
    return tds[0].get_text(strip=True) or None

LONGTERM_GRADE_RE = re.compile(
    r"^(AAA|AA\+|AA|AA\-|A\+|A|A\-|BBB\+|BBB|BBB\-|BB\+|BB|BB\-|B\+|B|B\-|CCC|CC|C|D)$"
)
OUTLOOK_RE = re.compile(r"(ì•ˆì •ì |ê¸ì •ì |ë¶€ì •ì |ìœ ë™ì |Stable|Positive|Negative|Developing)", re.I)

def _extract_grade_from_major_table(soup: BeautifulSoup) -> str | None:
    for tr in soup.find_all("tr"):
        text = tr.get_text(" ", strip=True)
        if "íšŒì‚¬ì±„" in text:
            toks = text.replace("/", " ").split()
            grades = [x for x in toks if LONGTERM_GRADE_RE.match(x)]
            outs   = [x for x in toks if OUTLOOK_RE.search(x)]
            if grades:
                g = grades[-1]
                return f"{g} {outs[-1]}" if outs else g
    return None

def _retry_get(session: requests.Session, url: str, max_retries=2, backoff=2) -> requests.Response:
    """500/502/503/504/429 ë“± ì¼ì‹œ ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„."""
    last_exc = None
    for attempt in range(1, max_retries + 1):
        try:
            resp = session.get(url, headers={**HEADERS, "Referer": HOME}, timeout=REQUEST_TIMEOUT)
            if resp.status_code in TRANSIENT_STATUS:
                raise requests.HTTPError(f"Transient {resp.status_code}", response=resp)
            resp.raise_for_status()
            return resp
        except requests.RequestException as e:
            last_exc = e
            st = getattr(e, "response", None).status_code if hasattr(e, "response") and e.response else None
            if st in TRANSIENT_STATUS and attempt < max_retries:
                print(f"âš ï¸ HTTP {st} ì¬ì‹œë„ {attempt}/{max_retries} â†’ {url}")
                time.sleep(backoff)
                continue
            break
    raise last_exc or RuntimeError("GET failed")

def fetch_company_and_grade_by_cmpcd(cmpCd: str, session: requests.Session) -> tuple[str, str]:
    """ìƒì„¸ í˜ì´ì§€(íšŒì‚¬ëª…/ì±„ê¶Œë“±ê¸‰) íŒŒì‹±."""
    url = f"{BASE}/disclosure/companyGradeInfo.do?cmpCd={cmpCd}&deviceType=N&isPaidMember=false"
    resp = _retry_get(session, url, max_retries=2, backoff=2)
    soup = BeautifulSoup(resp.text, "html.parser")

    name = _extract_name_from_tbl_type99(soup)
    if not name:
        for sel in ["div.cont_title h3", "h3.tit", ".company_name", ".cmp_name", "div.title_area h3"]:
            el = soup.select_one(sel)
            if el:
                name = el.get_text(strip=True)
                break
    if not name:
        title_text = soup.title.get_text(strip=True) if soup.title else ""
        name = title_text.split("|")[0].split("-")[0].strip() or cmpCd

    grade = _extract_grade_primary_tbl1(soup) or _extract_grade_from_major_table(soup) or ""
    return name, grade


# -------------------- ìºì‹œ(ë””ìŠ¤í¬ & ëŸ°íƒ€ì„) --------------------
DISK_CACHE = {"by_name": {}, "by_req": {}}
RUNTIME_CACHE_BY_NAME = {}   # { name_norm: {"íšŒì‚¬ëª…":..., "cmpCd":..., "ë“±ê¸‰":...} }
CACHE_LOCK = threading.Lock()

def _output_dir() -> str:
    base_dir = os.path.dirname(__file__)
    return os.path.join(base_dir, "..", "data", "output")

def _load_disk_cache() -> dict:
    """data/output/*.csv íŒŒì¼ë“¤ì„ ëª¨ì•„ ìºì‹œ ì¸ë±ìŠ¤ ìƒì„±."""
    out_dir = _output_dir()
    if not os.path.isdir(out_dir):
        return {"by_name": {}, "by_req": {}}

    csv_files = [f for f in os.listdir(out_dir) if f.lower().endswith(".csv")]
    if not csv_files:
        return {"by_name": {}, "by_req": {}}

    frames = []
    for fn in csv_files:
        path = os.path.join(out_dir, fn)
        try:
            df = pd.read_csv(path, encoding="utf-8-sig")
        except Exception:
            try:
                df = pd.read_csv(path, encoding="utf-8-sig", index_col=0).reset_index()
                if "index" in df.columns and "íšŒì‚¬ëª…" not in df.columns:
                    df = df.rename(columns={"index": "íšŒì‚¬ëª…"})
            except Exception:
                continue

        # í•„ìš”í•œ ì»¬ëŸ¼ ë³´ì •
        for col in ["íšŒì‚¬ëª…", "cmpCd", "ë“±ê¸‰", "ìš”ì²­ê²€ìƒ‰ì–´"]:
            if col not in df.columns:
                df[col] = None

        # íŒŒì¼ëª…ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹±(ì˜ˆ: ..._YYYYMMDD_HHMM.csv)
        ts = pd.Timestamp(1970, 1, 1)
        m = re.search(r"_(\d{8})[_-](\d{4})", fn)
        if m:
            try:
                ts = pd.to_datetime(m.group(1) + m.group(2), format="%Y%m%d%H%M")
            except Exception:
                pass
        df["source_ts"] = ts
        frames.append(df[["íšŒì‚¬ëª…", "cmpCd", "ë“±ê¸‰", "ìš”ì²­ê²€ìƒ‰ì–´", "source_ts"]])

    if not frames:
        return {"by_name": {}, "by_req": {}}

    cache_df = pd.concat(frames, ignore_index=True)
    cache_df["íšŒì‚¬ëª…"] = cache_df["íšŒì‚¬ëª…"].astype(str)
    cache_df["cmpCd"] = cache_df["cmpCd"].astype(str)
    cache_df.loc[cache_df["cmpCd"].isin(["", "nan", "None", "NaN"]), "cmpCd"] = None

    cache_df["name_norm"] = cache_df["íšŒì‚¬ëª…"].map(lambda s: normalize_text(aliasize(s)))
    if "ìš”ì²­ê²€ìƒ‰ì–´" in cache_df.columns:
        cache_df["req_norm"] = cache_df["ìš”ì²­ê²€ìƒ‰ì–´"].astype(str).map(lambda s: normalize_text(aliasize(s)))
    else:
        cache_df["req_norm"] = ""

    # ìµœì‹  í•­ëª©ì´ ë‚¨ë„ë¡ ì •ë ¬
    cache_df = cache_df.sort_values("source_ts")

    by_name, by_req = {}, {}
    for _, row in cache_df.iterrows():
        rec = {
            "íšŒì‚¬ëª…": row["íšŒì‚¬ëª…"],
            "cmpCd": row["cmpCd"],
            "ë“±ê¸‰": row["ë“±ê¸‰"],
            "source_ts": row["source_ts"],
        }
        if row["name_norm"]:
            by_name[row["name_norm"]] = rec
        if row["req_norm"]:
            by_req[row["req_norm"]] = rec

    return {"by_name": by_name, "by_req": by_req}

def _lookup_cache(query: str) -> dict | None:
    """ì¿¼ë¦¬ë¥¼ ìºì‹œì—ì„œ ì¡°íšŒ. ëŸ°íƒ€ì„ ìºì‹œ â†’ ë””ìŠ¤í¬ ìºì‹œ ìˆœ."""
    variants = [
        query,
        aliasize(query),
        query.replace(" ", ""),
        aliasize(query).replace(" ", ""),
    ]
    # 1) ëŸ°íƒ€ì„ ìºì‹œ(ì •í™• ì¼ì¹˜)
    with CACHE_LOCK:
        for v in variants:
            key = normalize_text(v)
            if key in RUNTIME_CACHE_BY_NAME:
                return RUNTIME_CACHE_BY_NAME[key]

    # 2) ë””ìŠ¤í¬ ìºì‹œ: ìš”ì²­ê²€ìƒ‰ì–´ ê¸°ì¤€(ì •í™• ì¼ì¹˜)
    for v in variants:
        key = normalize_text(v)
        if key in DISK_CACHE["by_req"]:
            return DISK_CACHE["by_req"][key]

    # 3) ë””ìŠ¤í¬ ìºì‹œ: íšŒì‚¬ëª… ê¸°ì¤€(ì •í™• ì¼ì¹˜)
    for v in variants:
        key = normalize_text(v)
        if key in DISK_CACHE["by_name"]:
            return DISK_CACHE["by_name"][key]

    # (ì„ íƒ) í¼ì§€ ë§¤ì¹­ì„ í—ˆìš©í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ + ì„ê³„ì¹˜ ì¡°ì •
    # if not CACHE_MATCH_EXACT_ONLY:
    #     # by_nameì—ì„œ ìœ ì‚¬ë„ ìµœê³  í•­ëª© ì„ íƒ(0.93 ì´ìƒ ê°€ì •)
    #     best_key, best_score, best_rec = None, 0.0, None
    #     for k, rec in DISK_CACHE["by_name"].items():
    #         s = name_similarity(k, normalize_text(query))
    #         if s > best_score:
    #             best_key, best_score, best_rec = k, s, rec
    #     if best_rec and best_score >= 0.93:
    #         return best_rec

    return None

def _update_runtime_cache(records: list[dict]) -> None:
    """ë™ì¼ ì‹¤í–‰ ì¤‘ ìˆ˜ì§‘í•œ ê²°ê³¼ë¥¼ ëŸ°íƒ€ì„ ìºì‹œì— ë°˜ì˜."""
    with CACHE_LOCK:
        for r in records:
            nm = normalize_text(aliasize(r["íšŒì‚¬ëª…"]))
            RUNTIME_CACHE_BY_NAME[nm] = {"íšŒì‚¬ëª…": r["íšŒì‚¬ëª…"], "cmpCd": r["cmpCd"], "ë“±ê¸‰": r["ë“±ê¸‰"]}


# -------------------- ëª©ë¡(í‘œ) íŒŒì„œ --------------------
CMP_RE = re.compile(r"cmpCd=(\d+)")
JS_CMP_RE = re.compile(r"fn_cmpGradeInfo\('(\d+)'\)")
ANY_CODE_RE = re.compile(r"(\d{7,8})")
GRADE_CELL_RE = re.compile(
    r"(AAA|AA\+|AA|AA-|A\+|A|A-|BBB\+|BBB|BBB-|BB\+|BB|BB-|B\+|B|B-|CCC|CC|C|D)"
    r"(?:\s*/\s*(Stable|Positive|Negative|Developing|ì•ˆì •ì |ê¸ì •ì |ë¶€ì •ì |ìœ ë™ì ))?",
    re.I
)

def _extract_candidates_from_search_table(html: str, require_bond=True) -> list[dict]:
    """
    /search/search.do ê²°ê³¼ í…Œì´ë¸”ì—ì„œ
    - cmpCd
    - name_hint
    - list_grade(ì±„ê¶Œ ì¹¼ëŸ¼ ê°’, ìˆìœ¼ë©´)
    ì¶”ì¶œ. require_bond=Falseë©´ 'ì±„ê¶Œ' ê°’ ì—†ì–´ë„ cmpCdë§Œ ìˆìœ¼ë©´ í›„ë³´ í¬í•¨.
    """
    soup = BeautifulSoup(html, "html.parser")
    table = soup.select_one("div.tbl_type01 table") or soup.find("table")
    if not table:
        return []

    out, tbody = [], (table.find("tbody") or table)
    for tr in tbody.find_all("tr", recursive=False):
        tds = tr.find_all("td")
        if not tds:
            continue

        # cmpCd
        cmpcd = None
        for a in tr.select('a[href]'):
            href = a.get("href") or ""
            m = CMP_RE.search(href) or JS_CMP_RE.search(href)
            if m:
                cmpcd = m.group(1); break
        if not cmpcd:
            for el in tr.find_all(onclick=True):
                oc = el.get("onclick") or ""
                m = JS_CMP_RE.search(oc) or CMP_RE.search(oc) or ANY_CODE_RE.search(oc)
                if m and len(m.group(1)) >= 7:
                    cmpcd = m.group(1); break
        if not cmpcd:
            for el in tr.find_all(True):
                for _, v in el.attrs.items():
                    if isinstance(v, str):
                        m = JS_CMP_RE.search(v) or CMP_RE.search(v) or ANY_CODE_RE.search(v)
                        if m and len(m.group(1)) >= 7:
                            cmpcd = m.group(1); break
                if cmpcd: break
        if not cmpcd:
            continue

        # ì´ë¦„ íŒíŠ¸
        name_hint = ""
        a_name = tr.select_one('a[href*="companyGradeInfo.do"], a[href^="javascript:fn_cmpGradeInfo"]')
        if a_name:
            name_hint = a_name.get_text(" ", strip=True)
        if not name_hint:
            for td in tds:
                txt = td.get_text(" ", strip=True)
                if txt:
                    name_hint = txt; break

        # 'ì±„ê¶Œ' ì¹¼ëŸ¼ ê°’(ìˆìœ¼ë©´)
        list_grade = None
        for td in tds:
            text = td.get_text(" ", strip=True)
            m = GRADE_CELL_RE.search(text)
            if m:
                g, o = m.group(1), m.group(2)
                list_grade = f"{g} {o}" if o else g
                break

        if require_bond and not list_grade:
            continue

        out.append({"cmpCd": cmpcd, "name_hint": name_hint, "list_grade": list_grade})

    return out


# -------------------- ê²€ìƒ‰ ì‹¤í–‰(ê²¬ê³ ) --------------------
def _stealth_options() -> Options:
    opts = Options()
    if HEADLESS: opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--lang=ko-KR")
    opts.add_argument("--window-size=1280,2000")
    opts.add_argument(f"user-agent={HEADERS['User-Agent']}")
    opts.add_experimental_option("excludeSwitches", ["enable-automation"])
    opts.add_experimental_option("useAutomationExtension", False)
    return opts

def _post_launch_stealth(driver):
    try:
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {"source": 'Object.defineProperty(navigator, "webdriver", {get: () => undefined});'}
        )
    except Exception:
        pass

def _parse_cmpcd_from_url(url: str) -> str | None:
    try:
        q = parse_qs(urlparse(url).query)
        if "cmpCd" in q and q["cmpCd"]:
            return q["cmpCd"][0]
    except Exception:
        pass
    m = re.search(r"cmpCd=(\d+)", url)
    return m.group(1) if m else None

def _build_search_url(query: str) -> str:
    return f"{BASE}/search/search.do?mainSType=CMP&mainSText={quote_plus(query)}"

def search_and_collect_resilient(
    driver: webdriver.Chrome,
    query: str,
    session: requests.Session,
    wait_timeout: int,
    table_wait_extra: float,
    require_bond_in_table: bool
) -> list[dict]:
    """
    - ê²€ìƒ‰ URLë¡œ ì§ì ‘ ì§„ì…
    - ìƒì„¸ë¡œ ë°”ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë˜ë©´ cmpCd ì¶”ì¶œ í›„ ìƒì„¸ íŒŒì‹±
    - ëª©ë¡(í‘œ)ë©´ í…Œì´ë¸” íŒŒì‹± â†’ (ì˜µì…˜) 'ì±„ê¶Œ' ì¹¼ëŸ¼ ê°’ ìˆëŠ” í–‰ë§Œ ìƒì„¸ ì¬íŒŒì‹±
    - DOM ëŠ¦ê²Œ ëœ° ìˆ˜ ìˆì–´ ì¬íŒŒì‹± 1íšŒ ë” ì‹œë„
    """
    results: list[dict] = []
    candidates_query = [
        query,
        aliasize(query),
        aliasize(query).replace(" ", ""),
        query.replace(" ", ""),
    ]
    seen_codes = set()

    for q in candidates_query:
        driver.get(_build_search_url(q))
        try:
            WebDriverWait(driver, wait_timeout).until(
                EC.any_of(
                    EC.url_contains("companyGradeInfo.do"),
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div.tbl_type01 table")),
                    EC.presence_of_element_located((By.CSS_SELECTOR, "table"))
                )
            )
        except Exception:
            pass

        # ìƒì„¸ë¡œ ë°”ë¡œ ì´ë™
        cmpcd = _parse_cmpcd_from_url(driver.current_url)
        if cmpcd:
            if cmpcd not in seen_codes:
                name, grade = fetch_company_and_grade_by_cmpcd(cmpcd, session)
                if grade:
                    seen_codes.add(cmpcd)
                    results.append({"ìš”ì²­ê²€ìƒ‰ì–´": query, "íšŒì‚¬ëª…": name, "cmpCd": cmpcd, "ë“±ê¸‰": grade})
                    _update_runtime_cache(results)  # ëŸ°íƒ€ì„ ìºì‹œ ë°˜ì˜
            return results  # ìƒì„¸ë©´ 1ê±´ìœ¼ë¡œ ì¢…ë£Œ

        # ëª©ë¡(í‘œ) íŒŒì‹±
        html = driver.page_source
        cands = _extract_candidates_from_search_table(html, require_bond=require_bond_in_table)
        if not cands:
            time.sleep(table_wait_extra)
            html = driver.page_source
            cands = _extract_candidates_from_search_table(html, require_bond=require_bond_in_table)

        if cands:
            for cand in cands:
                code = cand["cmpCd"]
                if code in seen_codes:
                    continue
                name, grade = fetch_company_and_grade_by_cmpcd(code, session)
                if not grade:
                    continue
                seen_codes.add(code)
                item = {"ìš”ì²­ê²€ìƒ‰ì–´": query, "íšŒì‚¬ëª…": name, "cmpCd": code, "ë“±ê¸‰": grade}
                results.append(item)
            if results:
                _update_runtime_cache(results)  # ëŸ°íƒ€ì„ ìºì‹œ ë°˜ì˜
            return results

        # ë‹¤ìŒ ë³€í˜•ìœ¼ë¡œ ê³„ì†
    return results


# -------------------- ë“œë¼ì´ë²„ ê´€ë¦¬ --------------------
def _new_driver(driver_path: str) -> webdriver.Chrome:
    options = _stealth_options()
    driver = webdriver.Chrome(service=Service(driver_path), options=options)
    _post_launch_stealth(driver)
    return driver


# -------------------- ë³‘ë ¬ ì›Œì»¤ --------------------
def worker_process(batch: list[str], worker_id: int, driver_path: str) -> dict:
    """
    ìŠ¤ë ˆë“œ 1ê°œê°€ ë°°ì¹˜(ê¸°ì—… ë¦¬ìŠ¤íŠ¸) ì²˜ë¦¬.
    - ìºì‹œ ìš°ì„  ì¡°íšŒ
    - ì„¸ì…˜ ê¹¨ì§€ë©´ ë“œë¼ì´ë²„ ì¬ìƒì„± + 1íšŒ ì¬ì‹œë„
    """
    rows, skipped = [], []
    driver = _new_driver(driver_path)
    session = requests.Session()
    session.headers.update(HEADERS)

    try:
        for i, q in enumerate(batch, 1):
            # 0) ìºì‹œ íˆíŠ¸ ì‹œ ì¦‰ì‹œ ë°˜í™˜(í¬ë¡¤ë§ ìƒëµ)
            if USE_CACHE_FIRST:
                cached = _lookup_cache(q)
                if cached and cached.get("ë“±ê¸‰"):
                    rows.append({
                        "ìš”ì²­ê²€ìƒ‰ì–´": q,
                        "íšŒì‚¬ëª…": cached["íšŒì‚¬ëª…"],
                        "cmpCd": cached.get("cmpCd"),
                        "ë“±ê¸‰": cached["ë“±ê¸‰"],
                        "source": "cache"
                    })
                    # ëŸ°íƒ€ì„ ìºì‹œë„ ìµœì‹ í™”
                    _update_runtime_cache([{"íšŒì‚¬ëª…": cached["íšŒì‚¬ëª…"], "cmpCd": cached.get("cmpCd"), "ë“±ê¸‰": cached["ë“±ê¸‰"]}])
                    if i % 10 == 0:
                        print(f"[W{worker_id}] ì§„í–‰ë¥ (ìºì‹œ): {i}/{len(batch)}")
                    continue  # ë‹¤ìŒ íšŒì‚¬ë¡œ

            # 1) í¬ë¡¤ë§
            retry_once = False
            while True:
                try:
                    found = search_and_collect_resilient(
                        driver, q, session,
                        wait_timeout=SEARCH_TIMEOUT,
                        table_wait_extra=0.8,
                        require_bond_in_table=REQUIRE_BOND_IN_TABLE_FIRST
                    )
                    if not found:
                        print(f"[W{worker_id}] ğŸ” ê²°ê³¼ ì—†ìŒ: {q}")
                        skipped.append((q, "ê²°ê³¼ ì—†ìŒ"))
                    else:
                        rows.extend(found)
                    break  # ì •ìƒ ì²˜ë¦¬ or ê²°ê³¼ ì—†ìŒ â†’ ë‹¤ìŒ íšŒì‚¬
                except WebDriverException as e:
                    msg = str(e).lower()
                    if not retry_once and ("invalid session id" in msg or "disconnected" in msg or "chrome not reachable" in msg):
                        print(f"[W{worker_id}] â™»ï¸ ë“œë¼ì´ë²„ ì¬ìƒì„± í›„ ì¬ì‹œë„: {q}")
                        try:
                            driver.quit()
                        except Exception:
                            pass
                        driver = _new_driver(driver_path)
                        retry_once = True
                        continue
                    else:
                        print(f"[W{worker_id}] â— ê²€ìƒ‰ ì‹¤íŒ¨: {q} -> {e}")
                        skipped.append((q, "ê²€ìƒ‰ ì‹¤íŒ¨"))
                        break
                except Exception as e:
                    print(f"[W{worker_id}] â— ê²€ìƒ‰ ì‹¤íŒ¨: {q} -> {e}")
                    skipped.append((q, "ê²€ìƒ‰ ì‹¤íŒ¨"))
                    break

            if i % 10 == 0:
                print(f"[W{worker_id}] ì§„í–‰ë¥ : {i}/{len(batch)}")
            time.sleep(0.25)  # ì„œë²„ ì˜ˆì˜
    finally:
        try:
            driver.quit()
        except Exception:
            pass

    return {"rows": rows, "skipped": skipped}


# -------------------- ìµœì¢… ë¦¬íŠ¸ë¼ì´(ì§ë ¬) --------------------
def retry_failed_serial(failed_names: list[str], driver_path: str) -> tuple[list[dict], list[tuple[str, str]]]:
    """
    1ë¼ìš´ë“œì—ì„œ 'ê²€ìƒ‰ ì‹¤íŒ¨/ê²°ê³¼ ì—†ìŒ'ì´ì—ˆë˜ ê¸°ì—…:
    - ìºì‹œ ë¨¼ì € í™•ì¸
    - ë‹¨ì¼ ë“œë¼ì´ë²„ë¡œ ì§ë ¬ ì¬ê²€ìƒ‰(ëŒ€ê¸° ëŠ˜ë¦¼)
    - í‘œì—ì„œ ì±„ê¶Œê°’ ì—†ì–´ë„ cmpCdë§Œ ìˆìœ¼ë©´ ìƒì„¸ ì¬íŒŒì‹±
    """
    rows, skipped = [], []
    if not failed_names:
        return rows, skipped

    print(f"\nğŸ” ìµœì¢… ë¦¬íŠ¸ë¼ì´ ëŒ€ìƒ ìˆ˜: {len(failed_names)}")
    driver = _new_driver(driver_path)
    session = requests.Session()
    session.headers.update(HEADERS)

    try:
        for idx, q in enumerate(failed_names, 1):
            # 0) ìºì‹œ ì¬í™•ì¸(1ë¼ìš´ë“œ ì¤‘ ë‹¤ë¥¸ ì›Œì»¤ê°€ ìˆ˜ì§‘í–ˆì„ ìˆ˜ë„ ìˆìŒ)
            cached = _lookup_cache(q)
            if cached and cached.get("ë“±ê¸‰"):
                rows.append({
                    "ìš”ì²­ê²€ìƒ‰ì–´": q,
                    "íšŒì‚¬ëª…": cached["íšŒì‚¬ëª…"],
                    "cmpCd": cached.get("cmpCd"),
                    "ë“±ê¸‰": cached["ë“±ê¸‰"],
                    "source": "cache(retry)"
                })
                _update_runtime_cache([{"íšŒì‚¬ëª…": cached["íšŒì‚¬ëª…"], "cmpCd": cached.get("cmpCd"), "ë“±ê¸‰": cached["ë“±ê¸‰"]}])
                continue

            try:
                found = search_and_collect_resilient(
                    driver, q, session,
                    wait_timeout=FINAL_RETRY_TIMEOUT,
                    table_wait_extra=FINAL_RETRY_SLEEP,
                    require_bond_in_table=REQUIRE_BOND_IN_TABLE_RETRY
                )
                if not found:
                    print(f"[RETRY] ì—¬ì „íˆ ê²°ê³¼ ì—†ìŒ: {q}")
                    skipped.append((q, "ìµœì¢… ê²°ê³¼ ì—†ìŒ"))
                else:
                    rows.extend(found)
                    _update_runtime_cache(found)
            except Exception as e:
                print(f"[RETRY] â— ì‹¤íŒ¨: {q} -> {e}")
                skipped.append((q, "ìµœì¢… ê²€ìƒ‰ ì‹¤íŒ¨"))

            if idx % 20 == 0:
                print(f"[RETRY] ì§„í–‰ë¥ : {idx}/{len(failed_names)}")
            time.sleep(0.35)
    finally:
        try:
            driver.quit()
        except Exception:
            pass

    return rows, skipped


# -------------------- ì…/ì¶œë ¥ & ë©”ì¸ --------------------
def load_companies_from_txt() -> list[str]:
    base_dir = os.path.dirname(__file__)
    path = os.path.join(base_dir, "..", "data", "input", "companies.txt")
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8-sig") as f:
        return [ln.strip() for ln in f if ln.strip()]

def save_dataframe(df: pd.DataFrame, suffix="parallel_cache") -> str:
    base_dir = os.path.dirname(__file__)
    out_dir = os.path.join(base_dir, "..", "data", "output")
    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, f"ë‚˜ì´ìŠ¤_íšŒì‚¬ì±„ë“±ê¸‰_by_search_{suffix}_{datetime.now():%Y%m%d_%H%M}.csv")
    df.to_csv(out_path, encoding="utf-8-sig")
    return out_path

def chunk_by_size(lst: list, size: int) -> list[list]:
    return [lst[i:i+size] for i in range(0, len(lst), size)]

# ì¶”ê°€: í”„ë¡œê·¸ë˜ë° í˜¸ì¶œìš© í•¨ìˆ˜ (ë¦¬ìŠ¤íŠ¸ ì…ë ¥ë°›ì•„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ë°˜í™˜)
def crawl_companies(companies: list[str]) -> tuple[pd.DataFrame, list[tuple[str, str]]]:
    """
    companies: list of company names (strings)
    returns: (df_final, skipped_all)
    """
    # 0) ë””ìŠ¤í¬ ìºì‹œ ë¡œë“œ (í•œ ë²ˆë§Œ)
    global DISK_CACHE
    DISK_CACHE = _load_disk_cache()
    print(f"ğŸ’¾ ë””ìŠ¤í¬ ìºì‹œ ë¡œë“œ ì™„ë£Œ: by_name={len(DISK_CACHE['by_name'])}, by_req={len(DISK_CACHE['by_req'])}")

    if not companies:
        raise ValueError("ì…ë ¥ëœ íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    print(f"ğŸ“‹ ëŒ€ìƒ ê²€ìƒ‰ì–´ ìˆ˜: {len(companies)}")

    driver_path = ChromeDriverManager().install()

    # ---- 1ë¼ìš´ë“œ: ë³‘ë ¬ ì²˜ë¦¬ ----
    if BATCH_SIZE_AUTO:
        size = max(1, math.ceil(len(companies) / MAX_WORKERS))
    else:
        size = 40
    batches = chunk_by_size(companies, size)
    print(f"ğŸ§µ ì›Œì»¤ ìˆ˜: {MAX_WORKERS}, ë°°ì¹˜ {len(batches)}ê°œ, ë°°ì¹˜ í¬ê¸° â‰ˆ {size}")

    rows_all, skipped_all = [], []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futs = {ex.submit(worker_process, batch, idx+1, driver_path): idx+1
                for idx, batch in enumerate(batches)}
        for fut in as_completed(futs):
            wid = futs[fut]
            try:
                res = fut.result()
                rows_all.extend(res["rows"])
                skipped_all.extend(res["skipped"])
                print(f"[W{wid}] âœ… ì™„ë£Œ: rows={len(res['rows'])}, skipped={len(res['skipped'])}")
            except Exception as e:
                print(f"[W{wid}] â— ì›Œì»¤ ì˜ˆì™¸: {e}")

    # ---- 2ë¼ìš´ë“œ: ìµœì¢… ë¦¬íŠ¸ë¼ì´(ì„ íƒ) ----
    if FINAL_RETRY:
        failed_names = [name for name, why in skipped_all if why in ("ê²€ìƒ‰ ì‹¤íŒ¨", "ê²°ê³¼ ì—†ìŒ")]
        failed_names = list(dict.fromkeys(failed_names))  # ì¤‘ë³µ ì œê±°, ìˆœì„œ ë³´ì¡´
        if failed_names:
            rows_retry, skipped_retry = retry_failed_serial(failed_names, driver_path)
            rows_all.extend(rows_retry)
            # ìµœì¢… ìŠ¤í‚µ ê°±ì‹ 
            skipped_all = [(n, w) for (n, w) in skipped_all if n not in failed_names] + skipped_retry

    # rows_all: ì• ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘í•œ ì›ì‹œ ê²°ê³¼ (list[dict])
    df_final = build_two_column_df(companies, rows_all)
    return df_final, skipped_all

def main():
    # argparseë¡œ í„°ë¯¸ë„ ì…ë ¥ ìˆ˜ì‹ ; ì…ë ¥ ì—†ìœ¼ë©´ ê¸°ì¡´ companies.txt ì‚¬ìš©(ê¸°ì¡´ ë™ì‘ ìœ ì§€)
    parser = argparse.ArgumentParser(description="NICE íšŒì‚¬ì±„ë“±ê¸‰ í¬ë¡¤ëŸ¬ - íšŒì‚¬ëª…ì„ ì¸ìë¡œ ë„˜ê¸°ê±°ë‚˜, ì…ë ¥ ì—†ìœ¼ë©´ companies.txt ì‚¬ìš©")
    parser.add_argument("companies", nargs="*", help="íšŒì‚¬ëª…ë“¤ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥ (ì˜ˆ: ì‚¼ì„±ì „ì SKí•˜ì´ë‹‰ìŠ¤)")
    args = parser.parse_args()
    companies = args.companies if args.companies else load_companies_from_txt()

    # 0) ë””ìŠ¤í¬ ìºì‹œ ë¡œë“œ (í•œ ë²ˆë§Œ)
    global DISK_CACHE
    DISK_CACHE = _load_disk_cache()
    print(f"ğŸ’¾ ë””ìŠ¤í¬ ìºì‹œ ë¡œë“œ ì™„ë£Œ: by_name={len(DISK_CACHE['by_name'])}, by_req={len(DISK_CACHE['by_req'])}")

    if not companies:
        print("âš  companies.txtê°€ ë¹„ì–´ìˆê±°ë‚˜ ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆìœ¼ë©°, í„°ë¯¸ë„ ì…ë ¥ë„ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    print(f"ğŸ“‹ ëŒ€ìƒ ê²€ìƒ‰ì–´ ìˆ˜: {len(companies)}")

    driver_path = ChromeDriverManager().install()

    # ---- 1ë¼ìš´ë“œ: ë³‘ë ¬ ì²˜ë¦¬ ----
    if BATCH_SIZE_AUTO:
        size = max(1, math.ceil(len(companies) / MAX_WORKERS))
    else:
        size = 40
    batches = chunk_by_size(companies, size)
    print(f"ğŸ§µ ì›Œì»¤ ìˆ˜: {MAX_WORKERS}, ë°°ì¹˜ {len(batches)}ê°œ, ë°°ì¹˜ í¬ê¸° â‰ˆ {size}")

    rows_all, skipped_all = [], []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futs = {ex.submit(worker_process, batch, idx+1, driver_path): idx+1
                for idx, batch in enumerate(batches)}
        for fut in as_completed(futs):
            wid = futs[fut]
            try:
                res = fut.result()
                rows_all.extend(res["rows"])
                skipped_all.extend(res["skipped"])
                print(f"[W{wid}] âœ… ì™„ë£Œ: rows={len(res['rows'])}, skipped={len(res['skipped'])}")
            except Exception as e:
                print(f"[W{wid}] â— ì›Œì»¤ ì˜ˆì™¸: {e}")

    # ---- 2ë¼ìš´ë“œ: ìµœì¢… ë¦¬íŠ¸ë¼ì´(ì„ íƒ) ----
    if FINAL_RETRY:
        failed_names = [name for name, why in skipped_all if why in ("ê²€ìƒ‰ ì‹¤íŒ¨", "ê²°ê³¼ ì—†ìŒ")]
        failed_names = list(dict.fromkeys(failed_names))  # ì¤‘ë³µ ì œê±°, ìˆœì„œ ë³´ì¡´
        if failed_names:
            rows_retry, skipped_retry = retry_failed_serial(failed_names, driver_path)
            rows_all.extend(rows_retry)
            # ìµœì¢… ìŠ¤í‚µ ê°±ì‹ 
            skipped_all = [(n, w) for (n, w) in skipped_all if n not in failed_names] + skipped_retry

    # rows_all: ì• ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘í•œ ì›ì‹œ ê²°ê³¼ (list[dict])
    df_final = build_two_column_df(companies, rows_all)

    # ì €ì¥ ê²½ë¡œ
    base_dir = os.path.dirname(__file__)
    out_dir = os.path.join(base_dir, "..", "data", "output")
    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, f"íšŒì‚¬ì±„ë“±ê¸‰_ìš”ì²­ëª…_2col_{datetime.now():%Y%m%d_%H%M}.csv")

    # ë‘ ì»¬ëŸ¼ë§Œ ì €ì¥
    df_final[["íšŒì‚¬ëª…", "ë“±ê¸‰"]].to_csv(out_path, index=False, encoding="utf-8-sig")

    print("\n[ë¯¸ë¦¬ë³´ê¸°] ìš”ì²­ëª…-ë“±ê¸‰ 2ì—´")
    print(df_final.head(10))
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {out_path}")

    # (ì„ íƒ) ìŠ¤í‚µ ë¡œê·¸
    if skipped_all:
        print("\nâ€” ê±´ë„ˆë›´ ê²€ìƒ‰ì–´(ì‚¬ìœ ) â€”")
        for name, why in skipped_all[:80]:
            print(f"  â€¢ {name}: {why}")
        if len(skipped_all) > 80:
            print(f"  â€¦ì™¸ {len(skipped_all)-80}ê±´")


if __name__ == "__main__":
    main()
