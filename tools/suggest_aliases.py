# tools/suggest_aliases.py
from __future__ import annotations

import argparse
import time
from collections import Counter, defaultdict
from pathlib import Path
from typing import Iterable, Dict, List

import numpy as np
import pandas as pd
from tqdm import tqdm

from kiwoom_finance.dart_client import init_dart, find_corp, extract_fs
from kiwoom_finance.preprocess import preprocess_all
from kiwoom_finance.aliases import _normalize_name, KOR_KEY_ALIASES

# ----------------------------
# ìœ í‹¸: ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜
# ----------------------------
def _safe_str(x) -> str:
    """ì»¬ëŸ¼ëª…ì´ float/NaN/Noneì´ì–´ë„ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜."""
    if x is None:
        return ""
    # pandasì˜ NaN / numpy.nan ì²˜ë¦¬
    if isinstance(x, float) and (np.isnan(x) or str(x) == "nan"):
        return ""
    s = str(x)
    if s.strip().lower() in {"nan", "none"}:
        return ""
    return s.strip()

# ----------------------------
# ì—°ê²°â†’ë³„ë„ ìë™ í´ë°±
# ----------------------------
def _extract_with_fallback(corp, bgn_de: str, report_tp: str, separate: bool):
    """
    1) separate ì„¤ì •ëŒ€ë¡œ ì‹œë„
    2) NotFoundConsolidated ì´ë©´ ë°˜ëŒ€ separateë¡œ ì¬ì‹œë„
    """
    try:
        return extract_fs(corp, bgn_de=bgn_de, report_tp=report_tp, separate=separate)
    except Exception as e:
        # dart_fssê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ë¬¸ìì—´ ë°©ì‹ë„ ë³‘í–‰
        is_nfc = "NotFoundConsolidated" in f"{type(e)} {e}"
        if not is_nfc:
            try:
                from dart_fss.errors.errors import NotFoundConsolidated  # type: ignore
                if isinstance(e, NotFoundConsolidated):
                    is_nfc = True
            except Exception:
                pass
        if is_nfc:
            alt = not separate
            tqdm.write(
                f"ğŸ” [{getattr(corp, 'stock_code', '???')}] ì—°ê²°/ë³„ë„ ë¯¸ë°œê²¬ â†’ "
                f"{'ë³„ë„' if alt else 'ì—°ê²°'}ë¡œ ì¬ì‹œë„"
            )
            return extract_fs(corp, bgn_de=bgn_de, report_tp=report_tp, separate=alt)
        raise

# ----------------------------
# ì €ì¥ í•¨ìˆ˜ (ì•ˆì „ ë¬¸ìì—´í™”)
# ----------------------------
def _save_candidates(counter: Counter, sample_map: Dict[str, List[str]], out_path: Path):
    rows = []
    for norm, cnt in counter.most_common():
        samples = sample_map.get(norm, [])[:5]
        # ëª¨ë‘ ë¬¸ìì—´í™” + ê³µë°±/NaN ì œê±° + ì¤‘ë³µ ì œê±°
        clean_samples = []
        for s in samples:
            ss = _safe_str(s)
            if ss and ss not in clean_samples:
                clean_samples.append(ss)
        examples_str = " | ".join(clean_samples)
        rows.append((norm, cnt, examples_str))

    df = pd.DataFrame(rows, columns=["normalized_name", "count", "examples"])
    out_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_path.as_posix(), index=False, encoding="utf-8-sig")
    return df

# ----------------------------
# ë©”ì¸ ìˆ˜ì§‘ ë¡œì§
# ----------------------------
def collect_unmatched_aliases(
    codes: Iterable[str],
    api_key: str | None = None,
    bgn_de: str = "20210101",
    report_tp: str = "annual",
    separate: bool = True,
    limit: int | None = None,
    throttle: float = 0.6,
    autosave_every: int = 100,
    quiet: bool = False,
):
    """
    ê° ì¢…ëª©ì˜ ì¬ë¬´ì œí‘œë¥¼ ìŠ¤ìº”í•˜ë©´ì„œ aliases.pyì— ì •ì˜ë˜ì§€ ì•Šì€ ë¯¸ë§¤ì¹­ ì»¬ëŸ¼ëª… í›„ë³´ë¥¼ ìˆ˜ì§‘.
    - ì—°ê²°â†’ë³„ë„ ìë™ í´ë°±
    - ì§„í–‰ë¥ /ETA í‘œì‹œ
    - ì£¼ê¸°ì  ìë™ì €ì¥
    """
    init_dart(api_key)

    # ì´ë¯¸ ë“±ë¡ëœ alias ì •ê·œí™” ì§‘í•©
    all_known = set(sum(KOR_KEY_ALIASES.values(), []))
    all_known_norm = {_normalize_name(x) for x in all_known if x}

    counter: Counter = Counter()
    sample_map: Dict[str, List[str]] = defaultdict(list)

    # ì¶œë ¥ ê²½ë¡œ
    project_root = Path(__file__).resolve().parents[1]
    out_dir = project_root / "artifacts"
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / "missing_aliases_candidates.csv"

    # ì§„í–‰ë¥  ë°”
    iter_codes = list(codes[:limit] if limit else codes)
    pbar = tqdm(total=len(iter_codes), ncols=110, dynamic_ncols=False, desc="ğŸ” Scanning aliases")

    ok = 0
    fail = 0
    t0 = time.time()
    last_start = t0

    for i, code in enumerate(iter_codes, start=1):
        start = time.time()
        try:
            time.sleep(throttle)
            corp = find_corp(code)
            fs = _extract_with_fallback(corp, bgn_de=bgn_de, report_tp=report_tp, separate=separate)
            bs, is_, cis, cf = preprocess_all(fs)

            for name, df in [("BS", bs), ("IS", is_), ("CIS", cis), ("CF", cf)]:
                if df is None or df.empty:
                    continue
                for c in df.columns:
                    # ì•ˆì „ ë¬¸ìì—´í™”
                    raw = _safe_str(c)
                    if not raw:
                        continue
                    norm = _normalize_name(raw)
                    if norm in all_known_norm:
                        continue
                    counter[norm] += 1
                    lst = sample_map[norm]
                    if raw not in lst and len(lst) < 8:
                        lst.append(raw)
            ok += 1
        except Exception as e:
            fail += 1
            if not quiet:
                tqdm.write(f"âš ï¸ {code}: {e}")
        finally:
            # ì§„í–‰ë¥  ìƒíƒœ ì—…ë°ì´íŠ¸
            now = time.time()
            last = now - start
            found = len(counter)
            pbar.set_postfix(
                last=f"{last:.2f}s", found=f"{found:,}", ok=ok, fail=fail
            )
            pbar.update(1)

            # ì£¼ê¸°ì  ìë™ì €ì¥
            if autosave_every and (i % autosave_every == 0):
                try:
                    _save_candidates(counter, sample_map, out_path)
                    if not quiet:
                        tqdm.write(f"ğŸ“ Autosaved â†’ {out_path}")
                except Exception as se:
                    tqdm.write(f"â— Autosave failed: {type(se).__name__}: {se}")

    pbar.close()

    # ìµœì¢… ì €ì¥
    df = _save_candidates(counter, sample_map, out_path)
    elapsed = time.time() - t0
    print(
        f"\nğŸ“ ì €ì¥ ì™„ë£Œ: {out_path}\n"
        f"   â€¢ distinct í›„ë³´: {len(counter):,}\n"
        f"   â€¢ ì²˜ë¦¬ ì¢…ëª©: {ok} ì„±ê³µ / {fail} ì‹¤íŒ¨\n"
        f"   â€¢ ì´ ê²½ê³¼: {elapsed:.1f}s (í‰ê·  {elapsed/max(1,len(iter_codes)):.2f}s/ì¢…ëª©)"
    )
    return df

# ----------------------------
# CLI
# ----------------------------
def _default_codes() -> List[str]:
    # ëŒ€í˜•ì£¼ ìœ„ì£¼ ìƒ˜í”Œ(í•„ìš” ì‹œ êµì²´)
    return [
        "086820","065170","456010","121890","122350",
        "069540","002420","241770","019170","005360",
        "032790","240810","013810","062040","005850",
        "266470","216050","361670","048870","189350",
        "018260","222080","102460","100590","217910",
    ]

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--api-key", default=None)
    ap.add_argument("--bgn-de", default="20210101")
    ap.add_argument("--report-tp", choices=["annual", "quarter"], default="annual")
    ap.add_argument("--separate", action="store_true", help="ì—°ê²° ëŒ€ì‹  ë³„ë„ ìš°ì„ ")
    ap.add_argument("--limit", type=int, default=None)
    ap.add_argument("--throttle", type=float, default=0.6, help="ìš”ì²­ ê°„ ëŒ€ê¸°(ì´ˆ). ë„ˆë¬´ ë‚®ì¶”ë©´ ì°¨ë‹¨ ìœ„í—˜")
    ap.add_argument("--autosave-every", type=int, default=100, help="Nê°œ ì²˜ë¦¬ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥")
    ap.add_argument("--quiet", action="store_true", help="ì˜ˆì™¸ ë¡œê·¸ ìµœì†Œí™”")
    ap.add_argument("--codes", nargs="*", default=[])
    args = ap.parse_args()

    codes = args.codes or _default_codes()
    collect_unmatched_aliases(
        codes=codes,
        api_key=args.api_key,
        bgn_de=args.bgn_de,
        report_tp=args.report_tp,
        separate=args.separate,
        limit=args.limit,
        throttle=args.throttle,
        autosave_every=args.autosave_every,
        quiet=args.quiet,
    )
